\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[english,man]{apa6}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Language Input to Blind Infants/Toddlers},
            pdfauthor={Erin Campbell1, Lillianna Writer1, \& Elika Bergelson1},
            pdfkeywords={keywords},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\begin{center}\begin{threeparttable}}
%   {\end{threeparttable}\end{center}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother
\shorttitle{Language Input to Blind Infants/Toddlers}
\keywords{keywords\newline\indent Word count: X}
\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=english]{babel}
\else
  % load polyglossia as late as possible as it *could* call bidi if RTL lang (e.g. Hebrew or Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\fi

\title{Language Input to Blind Infants/Toddlers}
\author{Erin Campbell\textsuperscript{1}, Lillianna Writer\textsuperscript{1}, \& Elika Bergelson\textsuperscript{1}}
\date{}


\note{

\textbf{Conflicts of Interest}: The authors have no conflicts of interest to report.
\textbf{Funding}: This work was supported by the National Science Foundation CAREER grant (BCS-1844710) to EB and Graduate Research Fellowship (2019274952) to EC.

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Department of Psychology \& Neuroscience, Duke University, Durham, NC}

\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The early language skills of blind children are highly variable ({\textbf{???}}), with some blind children demonstrating age-appropriate vocabulary from the earliest stages of language learning ({\textbf{???}}; Landau \& Gleitman, 1985), while others experience large and persistent language delays ({\textbf{???}}). The causes of this variability remain poorly understood, but the higher incidence of severe language delays in this population yields questions about the process of language development in the absence of visual perception: what contributes to variable language outcomes among young blind children? There are multiple possible contributors, including characteristics of the child (e.g., visual characteristics, comorbid conditions, gender) as well as characteristics of the environment (e.g., access to early intervention services; school setting). Here, we explore the characteristics of the language environment of blind children and its influence on language development.

Among both typically-developing children and children with developmental differences, language input is an important predictor of language outcomes ({\textbf{???}}; Anderson, Graham, Prime, Jenkins, \& Madigan, 2021; Anderson et al., 2021; Gilkerson et al., 2018; Huttenlocher, Waterfall, Vasilyeva, Vevea, \& Hedges, 2010; Rowe, 2008, 2012). There are many ways to operationalize language input, that tend to be grouped into \textbf{quantity of language input} and \textbf{quality of language input}. Quantity of language input can be operationalized as the number of words or utterances a child is exposed to. At a coarse level, children who are exposed to more speech (or sign ({\textbf{???}})) tend to have better language outcomes ({\textbf{???}}; Anderson et al., 2021; Gilkerson et al., 2018; Rowe, 2008). However, if only the \emph{amount} of language exposure mattered, then infants should be able to sit in front of the television all day and become fluent language users. Yet young children struggle to learn language from video (e.g., Roseberry, Hirsh-Pasek, \& Golinkoff, n.d.)

Language input quality is perhaps even more important ({\textbf{???}}; Rowe, 2012), though somewhat trickier to operationalize. Rowe and Snow (Rowe \& Snow, 2020) divide this space into three dimensions of language input quality: interactive features (e.g., parent responsiveness, speech directed \emph{to} child vs.~overheard; conversational turn-taking), linguistic features (e.g., lexical diversity, grammatical complexity), and conceptual features (e.g., topic diversity). These features interact with the child's present cogntive, linguistic, and conceptual abilities.

Interactiveness in parent-child communication is an important element: back-and-forth communicative exchanges predict better language learning across infancy (Donnellan, Bannard, McGillion, Slocombe, \& Matthews, 2020; Goldstein \& Schwade, 2008) and toddlerhood ({\textbf{???}}; Romeo et al., 2018). Another aspect of interactiveness is attuning to children's cues of attention and interest, like pointing or eye gaze. In infancy, words heard in these contexts are more likely to be learned ({\textbf{???}}; {\textbf{???}}). This interacts heavily with conceptual features of the language input. Conceptual supportive features of language input involve the relationship between conversational topics and the child's cognitive level. For example, infants are more likely to learn words when the object is perceptually salient, dominating their field of view ({\textbf{???}}). By contrast, in toddlerhood, parents' decontextualized language use (e.g., past/future events) predicts kindergarten vocabulary (Rowe, 2012), children's decontextualized language use ({\textbf{???}}), and academic achievement in adolescence ({\textbf{???}}).

In terms of linguistic quality, two common ways to quantify it are lexical diversity (often type/token ratio) and syntactic complexity. Lexical diversity of language input seems to exert different effects as children get older. In early infancy, children who are exposed to more repetitions at 7 months have higher vocabulary at age 2 ({\textbf{???}}). This relationship later flips: toddlers who are exposed to greater diversity of words in their language input tend to have larger vocabulary scores ({\textbf{???}}; {\textbf{???}}; Anderson et al., 2021; Huttenlocher et al., 2010; Rowe, 2012). Lexical diversity is intertwined with input quantity: parents who talk more also tend to provide more lexical diversity (Hoff \& Naigles, n.d.). Likewise, the diversity of syntactic constructions in parental language input is associated both with children's vocabulary growth and utterance structure diversity ({\textbf{???}}; {\textbf{???}}; {\textbf{???}}; {\textbf{???}}; {\textbf{???}}; Huttenlocher et al., 2010).

For blind children, language input may play an even more important role ({\textbf{???}}). In the absence of visual input, language is an important source of information about the world. Linguistic structure provides cues to word meaning that may be lost without visual cues (e.g., such as joint (visual) attention or pointing). All that said, language input may differ for blind children relative to sighted children

Speakers regularly tailor input to communicate efficiently with the listener ({\textbf{???}}). Parents are sensitive to their child's developmental level and tune language input accordingly ({\textbf{???}}; {\textbf{???}}). Child-directed speech is one example--whereby parents speak to young children with exaggerated prosody, slower speech rate, and increased vowel clarity ({\textbf{???}}; Bernstein Ratner, 1984), which appears to be helpful to the young language learner ({\textbf{???}}). Parents show increased alignment (a tendency to re-use use the conversation partner's expressions) for younger children, that decreases as children get older (Yurovsky, Doyle, \& Frank, 2016). When interacting with infants and toddlers, parents repeat words more often than when interacting with older children or adults ({\textbf{???}}). Communicative tailoring is also common in language input to children with disabilities, who tend to receive simplified, more directive language input, and less interactive input compared to typically-developing children ({\textbf{???}}; Yoshinaga-Itano, Sedey, Mason, Wiggin, \& Chung, 2020).

In addition to tailoring communication to children's developmental level, speakers also adjust their conversation in accordance with the listener's and visual access (Gergle, Kraut, \& Fussell, 2004; Grigoroglou, Edu, \& Papafragou, 2016). Speakers aim to provide the information their listeners lack but avoiding redundant visual description ({\textbf{???}}; Ostarek, Paridon, \& Montero-Melis, 2019). During in-lab tasks with sighted participants, participants tailor their descriptions and requests by verbally providing visually-absent cues when an object is occluded to their partner ({\textbf{???}}; Hawkins, Gweon, \& Goodman, 2021; Rubio-Fernandez, 2019). These results suggest that adults and even infants (Chiesa, Galati, \& Schmidt, 2015; Ganea et al., 2018; Senju et al., 2013) can flexibly adapt communication to the visual abilities of their partner.

Curiously though, these results aren't borne out in the existing literature on interactions between blind infants and their sighted parents. We might expect parents to verbally compensate for missing visual input, resulting in parents providing more description of the child's environment. Instead, caregivers of blind seem to restrict conversation to things that the blind child is currently engaged with, rather than attempt to redirect their attention to other stimuli ({\textbf{???}}; {\textbf{???}}; Kekelis \& Andersen, 1984). In naturalistic settings, parents of blind children use \emph{fewer} declaratives and \emph{more} imperatives and requests for actions/labels than parents of sighted children, suggesting that children might be receiving less description than sighted children (Kekelis \& Andersen, 1984; Landau \& Gleitman, 1985). That said, we do see some evidence for parents adapting to their child's visual abilities. ({\textbf{???}}) and colleagues find that in a more structured book reading task, parents of blind children provide more descriptive utterances than parents of sighted children. Further, parents of blind children provide more tactile cues to initiate interactions or establish joint attention ({\textbf{???}}; {\textbf{???}}). These mixed results suggest that parents of blind children might alter language input in some domains but not others.

Better understanding language how perceptual and linguistic input interact and influence children's language outcomes is of clinical and scientific relevance. Based on researchers' interactions with participants' families in the present study, parents are looking for evidence-based guidance to help them support their children's language development. If properties of language input influence the likelihood of language delays among blind infants/toddlers ({\textbf{???}}), then communicating this to families could help children reach their full potential. By contrast, if there is no relationship between language input properties and children's language outcomes, then perhaps language input is one fewer worry for caregivers. In the present study, we examine daylong recordings of naturalistic at-home language interactions between caregivers and their blind or sighted children. In order to understand whether parents speak differently to blind children than to sighted children, we first measure input along the dimensions of quantity (adult word count) and quality, split into interactiveness (conversational turn counts, proportion of child-directed vs.~adult-directed speech), conceptual features (topic diversity, tense, adjective typicality, sensory modality), and linguistic features (type/token ratio, mean length of utterance). We then link these features of language input to language outcomes, exploring whether the effects of parent language input on child language vary as a function of children's perceptual ability.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

15 blind infants and their families participated in this study. Blind participants were recruited through opthamologist referral, preschools, early intervention programs, social media, and word of mouth. To be eligible for this study, participants had to be 6--30 months old, have no additional disabilities (developmental delays; intellectual disabilities, or hearing loss), and be exposed to \(\geq\) 75\% English at home. Given the wide age range of the study, to control for age, each blind participant was matched to a sighted partcipicant, based on age (\(\pm\) 6 weeks), gender, maternal education (\(\pm\) one education level: less than high school diploma, high school diploma, some college / Associate's, Bachelor's, graduate school), and number of siblings (\(\pm\) 1 sibling). When more than one match was available, we prioritized matching the blind participants as closely as possible on each characteristics in the preceding order. Caregivers were asked to complete a demographics survey and the MacArthur-Bates Communicative Development Inventory (CDI; Fenson et al., 1994) within one week of the home language recording. See XXX for sample characteristics.

\hypertarget{recording-procedure}{%
\subsection{Recording Procedure}\label{recording-procedure}}

Eligible families were asked to complete two surveys and complete a daylong home language recording. For the recording portion of the study, caregivers of participating infants received a LENA wearable audio recorder (Ganek \& Eriks-Brophy, 2016) and vest. They were instructed to place the recorder in the vest on the day of their scheduled recording and put the vest on their child from the time they woke up until the recorder automatically shut off after 16 hours (setting vest nearby during bath, nap, and car times). They were also instructed how to pause the recording at any time, but asked to keep these pauses to a minimum. Actual recording length ranged from RANGE (XXX mean, SD).

\hypertarget{processing}{%
\subsection{Processing}\label{processing}}

Audio recordings were processed by LENA software (gives you an its? idk). Each recording was then run through an automated sampler that selected 15- non-overlapping 5-minute segments, randomly distributed across the duration of the file. The process output a codeable ELAN file (.eaf, CITE). Each segment consists of 2 core minutes of annotated time, with 2 minutes of listenable context marked out preceding the annotation clip and 1 minute of additional context following the annotation clip. Each file therefore contains 30 minutes of coded recording time and 75 minutes of total time listened (\#isn't there one where that's not true??) Because these segments were sampled randomly, and not on a high-volubility measure such as conversational turns or adult speech density, the amount of time with codeable speech input varied for each recording. Indeed, across participants (FIND A WAY TO DO MATH WITH \# SEGMENTS THAT ARE SILENT) of the 2-minute coding segments contained no speech at all.

\hypertarget{annotation}{%
\subsection{Annotation}\label{annotation}}

Trained annotators listened through each 2-minute segment plus its surrounding context and coded it using the Analyzing Child Language Experiences around the World (ACLEW) Daylong Audio Recording of Children's Linguistic Environments (DARCLE) annotation scheme (Soderstrom et al., 2021). Prior to annotating lab data, annotators are trained on previously coded samples of child recordings and are required to reach 95\% overall agreement with the gold standard version of the file for three different age ranges: 0-7 months, 8-18 months, and 19-36 months. For more information about this annotation scheme and the larger project, please see the ACLEW homepage (\url{https://sites.google.com/view/aclewdid/home}). Following the first pass, all files were checked by a highly-trained \enquote{superchecker} (second author on this paper, Lilli \enquote{Always Right} Righter) to ensure the consistency of annotations. (are we gonna do reliability? I don't want to lol)

This annotation scheme is designed to capture both utterances by the target child (henceforth referred to as CHI) and speech in the child's environment, including adults, other children, and pre-recorded electronic speech (e.g.~toys, television, the radio). Annotators segment the duration of each utterance on a separate coding tier for each unique speaker (exceptions: all electronic speech is coded on the same tier, and some speakers who appear briefly in these files were not easily distinguishable from others by annotators naive to their identities, so they may be concatenated on the same tier). Speech by people other than the target child is transcribed using an adapted version of CHAT transcription style (MacWhinney, 2019), dubbed minCHAT for the ACLEW project (Soderstrom et al., 2021). Because the majority of target children in the project are pre-lexical or phonetically immature, CHI utterances are not transcribed.

Each utterance is coded for additional linguistic properties from a set of pre-determined categories. CHI utterances are coded for vocal maturity, lexical status, and multi-word status. Vocal maturity classifies utterances into the following categories: laughing; crying; canonical syllables that contain a consonant-like and vowel-like sound component, including both babbling and identifiable words; non-canonical syllables, which do not contain both consonant and vowel portions, or which do not transition between them in a speech-like way; and unsure, when the vocalization type is unclear. Each vocalization that contains canonical syllables is then coded for lexical status, either containing an identifiable lexical item or not. Finally, each utterance with a lexical item is coded for multi-word status, whether or not it contains more than one unique word type.

Environmental speech is coded for the addressee of each utterance: speech directed to a child, whether or not it is directed to the target child; adult-directed speech; speech directed to both an adult and a child; speech directed to pets or other animals; unclear addressee; or speech directed towards a recipient that doesn't fit into another category (e.g.~voice control of Siri or Alexa, speech to a metaphysical entity).

Following ACLEW DARCLE style annotation (Soderstrom et al., 2021), each file was converted into a CHAT file (MacWhinney, 2018) to use the CLAN automated mean length of utterance (MLU) analysis for each speaker. This analysis finds the average number of morphemes per utterance, using the eng MOR grammar dictionary (MacWhinney, 2018).

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{measuring-properties-of-language-input}{%
\subsection{Measuring Properties of Language Input}\label{measuring-properties-of-language-input}}

\hypertarget{language-input-quantity}{%
\subsubsection{Language Input Quantity}\label{language-input-quantity}}

We first compare the quantity of language input to blind and sighted children, using LENA's automated Adult Word Count measure. A wilcoxon rank sum test shows that despite wide variability in the number of words children hear (Range: 6233--31745 words\textsubscript{blind}, 6027--25500 words\textsubscript{sighted}), blind and sighted children do not differ in language input quantity (W = 43, \emph{p} = .907).
\includegraphics{input_quality_manuscript_files/figure-latex/AWC-plot-1.pdf}

\hypertarget{language-input-quality}{%
\subsubsection{Language Input Quality}\label{language-input-quality}}

\hypertarget{interactiveness}{%
\paragraph{Interactiveness}\label{interactiveness}}

\includegraphics{input_quality_manuscript_files/figure-latex/ADS-CDS-props-1.pdf}

\includegraphics{input_quality_manuscript_files/figure-latex/unnamed-chunk-3-1.pdf}
\includegraphics{input_quality_manuscript_files/figure-latex/cute-plot-1.pdf}

\begin{verbatim}
## 
##  2-sample test for equality of proportions with continuity correction
## 
## data:  c(TD_CDS, VI_CDS) out of c(TD_mega_total, mega_total)
## X-squared = 7.49, df = 1, p-value = 0.006204
## alternative hypothesis: two.sided
## 95 percent confidence interval:
##  -0.06898762 -0.01146060
## sample estimates:
##    prop 1    prop 2 
## 0.4146032 0.4548273
\end{verbatim}

\begin{verbatim}
## 
##  2-sample test for equality of proportions with continuity correction
## 
## data:  c(TD_ADS, VI_ADS) out of c(TD_mega_total, mega_total)
## X-squared = 0.41327, df = 1, p-value = 0.5203
## alternative hypothesis: two.sided
## 95 percent confidence interval:
##  -0.03663111  0.01805645
## sample estimates:
##    prop 1    prop 2 
## 0.3250794 0.3343667
\end{verbatim}

\begin{verbatim}
## 
##  2-sample test for equality of proportions with continuity correction
## 
## data:  c(TD_ODS, VI_ODS) out of c(TD_mega_total, mega_total)
## X-squared = 49.494, df = 1, p-value = 1.99e-12
## alternative hypothesis: two.sided
## 95 percent confidence interval:
##  0.05508856 0.10291361
## sample estimates:
##    prop 1    prop 2 
## 0.2355556 0.1565545
\end{verbatim}

\includegraphics{input_quality_manuscript_files/figure-latex/addressee-prop-graphs-1.pdf}

We compared the proportions of child-directed speech (CDS) and adult-directed speech (ADS) between the blind children and their sighted matches. Each proportion was calculated as the number of utterances produced by someone \emph{other} than the target child (non-CHI utterances) tagged with a child or an adult addressee, respectively, out of the total number of non-CHI utterances for each sensory group. A two-sample test for equality of proportions revealed no significant difference in the overall proportions of CDS to blind children and CDS to sighted children (\emph{X}\^{}2=7.49, p=.006, CDS-proportion\textsubscript{blind}=0.45, CDS-proportion\textsubscript{sighted}=0.41). Likewise, there was no difference between the proportion of ADS to blind or sighted children (\emph{X}\^{}2=0.41, p=.520, ADS-proportion\textsubscript{blind}=0.33, ADS-proportion\textsubscript{sighted}=0.33).

We next compare the number of conversational turn counts for blind and sighted children, using LENA's automated Conversational Turn Count measure. A wilcoxon rank sum test shows that despite wide variability in the number conversational turns (210--1436 words\textsubscript{blind}, 112--672 words\textsubscript{sighted}), blind and sighted children do not differ in the number of conversational turns (W = 42, \emph{p} = .846).

\includegraphics{input_quality_manuscript_files/figure-latex/CTC-1.pdf}

\hypertarget{conceptual-features}{%
\paragraph{Conceptual Features}\label{conceptual-features}}

topic diversity
adjective typicality
sensory modality
tense

\hypertarget{linguistic-features}{%
\paragraph{Linguistic Features}\label{linguistic-features}}

type/token ratio
MLU

\hypertarget{linking-language-input-to-language-outcomes}{%
\subsection{Linking Language Input to Language Outcomes}\label{linking-language-input-to-language-outcomes}}

Predict: CDI percentile \& CVC percentile

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

Sighted parents may be unfamiliar with blind children's signals of interest and engagement ({\textbf{???}}), and as a result, may respond less often to infants' vocalizations and bids for communication ({\textbf{???}}). Might be hard to provide useful input due to differences in nonverbal communication between blind infants and their sighted caregivers. Young children born with visual impairment may differ in their nonverbal communication cues. For example, ({\textbf{???}}) found that 6--9-month-old blind infants communicated using leaning, eyebrow raising, and lip movements. Caregivers who responded to these nonverbal cues as conversational turns had higher rates of interaction with the child, higher rates of appropriate response, and increased positive affect. By contrast, caregivers who did not recognize these signals as communicative had lower rates of response and increased negative affect.

\pagebreak

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-anderson2021}{}%
Anderson, N. J., Graham, S. A., Prime, H., Jenkins, J. M., \& Madigan, S. (2021). Linking Quality and Quantity of Parental Linguistic Input to Child Language Skills: A Meta-Analysis. \emph{Child Development}, \emph{92}(2), 484--501. \url{https://doi.org/10.1111/cdev.13508}

\leavevmode\hypertarget{ref-bernsteinratner1984}{}%
Bernstein Ratner, N. (1984). Patterns of vowel modification in motherChild speech. \emph{Journal of Child Language}, \emph{11}, 557--578.

\leavevmode\hypertarget{ref-chiesa2015}{}%
Chiesa, S., Galati, D., \& Schmidt, S. (2015). Communicative interactions between visually impaired mothers and their sighted children: Analysis of gaze, facial expressions, voice and physical contacts. \emph{Child: Care, Health and Development}, \emph{41}(6), 1040--1046. \url{https://doi.org/10.1111/cch.12274}

\leavevmode\hypertarget{ref-donnellan2020}{}%
Donnellan, E., Bannard, C., McGillion, M. L., Slocombe, K. E., \& Matthews, D. (2020). Infants' intentionally communicative vocalizations elicit responses from caregivers and are the best predictors of the transition to language: A longitudinal investigation of infants' vocalizations, gestures and word production. \emph{Developmental Science}, \emph{23}(1), e12843. \url{https://doi.org/10.1111/desc.12843}

\leavevmode\hypertarget{ref-fenson1994}{}%
Fenson, L., Dale, P. S., Reznick, J. S., Bates, E., Thal, D. J., Pethick, S. J., \ldots{} Stiles, J. (1994). Variability in Early Communicative Development. \emph{Monographs of the Society for Research in Child Development}, \emph{59}(5), i. \url{https://doi.org/10.2307/1166093}

\leavevmode\hypertarget{ref-ganea2018}{}%
Ganea, N., Hudry, K., Vernetti, A., Tucker, L., Charman, T., Johnson, M. H., \& Senju, A. (2018). Development of adaptive communication skills in infants of blind parents. \emph{Developmental Psychology}, \emph{54}(12), 2265--2273. \url{https://doi.org/10.1037/dev0000564}

\leavevmode\hypertarget{ref-ganek2016}{}%
Ganek, H., \& Eriks-Brophy, A. (2016). The Language ENvironment Analysis (LENA) system: A literature review. In \emph{Proceedings of the joint workshop on NLP for Computer Assisted Language Learning and NLP for Language Acquisition} (pp. 24--32). Umeå, Sweden: LiU Electronic Press.

\leavevmode\hypertarget{ref-gergle2004}{}%
Gergle, D., Kraut, R. E., \& Fussell, S. R. (2004). Language Efficiency and Visual Technology: Minimizing Collaborative Effort with Visual Information. \emph{Journal of Language and Social Psychology}, \emph{23}(4), 491--517. \url{https://doi.org/10.1177/0261927X04269589}

\leavevmode\hypertarget{ref-gilkerson2018}{}%
Gilkerson, J., Richards, J. A., Warren, S. F., Oller, D. K., Russo, R., \& Vohr, B. (2018). Language Experience in the Second Year of Life and Language Outcomes in Late Childhood. \emph{Pediatrics}, \emph{142}(4), e20174276. \url{https://doi.org/10.1542/peds.2017-4276}

\leavevmode\hypertarget{ref-goldstein2008}{}%
Goldstein, M. H., \& Schwade, J. A. (2008). Social feedback to infants' babbling facilitates rapid phonological learning. \emph{Psychological Science}, \emph{19}(5), 515--523. \url{https://doi.org/10.1111/j.1467-9280.2008.02117.x}

\leavevmode\hypertarget{ref-grigoroglou2016}{}%
Grigoroglou, M., Edu, U., \& Papafragou, A. (2016). Are children flexible speakers? Effects of typicality and listener needs in children's event descriptions. \emph{Cognitive Science}, 6.

\leavevmode\hypertarget{ref-hawkins2021}{}%
Hawkins, R. D., Gweon, H., \& Goodman, N. D. (2021). The Division of Labor in Communication: Speakers Help Listeners Account for Asymmetries in Visual Perspective. \emph{Cognitive Science}, \emph{45}(3), e12926. \url{https://doi.org/10.1111/cogs.12926}

\leavevmode\hypertarget{ref-hoff2002}{}%
Hoff, E., \& Naigles, L. (n.d.). How children use input to acquire a lexicon. \emph{Child Development}, \emph{73}(2), 418--433. \url{https://doi.org/10.1111/1467-8624.00415}

\leavevmode\hypertarget{ref-huttenlocher2010}{}%
Huttenlocher, J., Waterfall, H., Vasilyeva, M., Vevea, J., \& Hedges, L. V. (2010). Sources of variability in children's language growth. \emph{Cognitive Psychology}, \emph{61}(4), 343--365. \url{https://doi.org/10.1016/j.cogpsych.2010.08.002}

\leavevmode\hypertarget{ref-kekelis1984}{}%
Kekelis, L. S., \& Andersen, E. S. (1984). Family Communication Styles and Language Development. \emph{Journal of Visual Impairment \& Blindness}, \emph{78}(2), 54--65. \url{https://doi.org/10.1177/0145482X8407800202}

\leavevmode\hypertarget{ref-landau1985}{}%
Landau, B., \& Gleitman, L. R. (1985). \emph{Language and experience: Evidence from the blind child} (pp. xi, 250). Cambridge, MA, US: Harvard University Press.

\leavevmode\hypertarget{ref-macwhinney2018}{}%
MacWhinney, B. (2018). CLAN Manual. \url{https://doi.org/10.21415/T5G10R}

\leavevmode\hypertarget{ref-macwhinney2019}{}%
MacWhinney, B. (2019). CHAT Manual. \url{https://doi.org/10.21415/3MHN-0Z89}

\leavevmode\hypertarget{ref-ostarek2019}{}%
Ostarek, M., Paridon, J. van, \& Montero-Melis, G. (2019). Sighted people's language is not helpful for blind individuals' acquisition of typical animal colors. \emph{Proceedings of the National Academy of Sciences}, \emph{116}(44), 21972--21973. \url{https://doi.org/10.1073/pnas.1912302116}

\leavevmode\hypertarget{ref-romeo2018}{}%
Romeo, R. R., Leonard, J. A., Robinson, S. T., West, M. R., Mackey, A. P., Rowe, M. L., \& Gabrieli, J. D. E. (2018). Beyond the 30-Million-Word Gap: Children's Conversational Exposure Is Associated With Language-Related Brain Function. \emph{Psychological Science}, \emph{29}(5), 700--710. \url{https://doi.org/10.1177/0956797617742725}

\leavevmode\hypertarget{ref-roseberry2014}{}%
Roseberry, S., Hirsh-Pasek, K., \& Golinkoff, R. M. (n.d.). Skype me! Socially contingent interactions help toddlers learn language. \emph{Child Development}, \emph{85}(3), 956--970. \url{https://doi.org/10.1111/cdev.12166}

\leavevmode\hypertarget{ref-rowe2008}{}%
Rowe, M. L. (2008). Child-directed speech: Relation to socioeconomic status, knowledge of child development and child vocabulary skill*. \emph{Journal of Child Language}, \emph{35}(1), 185--205. \url{https://doi.org/10.1017/S0305000907008343}

\leavevmode\hypertarget{ref-rowe2012}{}%
Rowe, M. L. (2012). A Longitudinal Investigation of the Role of Quantity and Quality of Child-Directed Speech in Vocabulary Development. \emph{Child Development}, \emph{83}(5), 1762--1774. \url{https://doi.org/10.1111/j.1467-8624.2012.01805.x}

\leavevmode\hypertarget{ref-rowe2020}{}%
Rowe, M. L., \& Snow, C. E. (2020). Analyzing input quality along three dimensions: Interactive, linguistic, and conceptual. \emph{Journal of Child Language}, \emph{47}(1), 5--21. \url{https://doi.org/10.1017/S0305000919000655}

\leavevmode\hypertarget{ref-rubio-fernandez2019}{}%
Rubio-Fernandez, P. (2019). Overinformative Speakers Are Cooperative: Revisiting the Gricean Maxim of Quantity. \emph{Cognitive Science}, \emph{43}(11), e12797. \url{https://doi.org/10.1111/cogs.12797}

\leavevmode\hypertarget{ref-senju2013}{}%
Senju, A., Tucker, L., Pasco, G., Hudry, K., Elsabbagh, M., Charman, T., \& Johnson, M. H. (2013). The importance of the eyes: Communication skills in infants of blind parents. \emph{Proceedings. Biological Sciences}, \emph{280}(1760), 20130436. \url{https://doi.org/10.1098/rspb.2013.0436}

\leavevmode\hypertarget{ref-soderstrom2021}{}%
Soderstrom, M., Casillas, M., Bergelson, E., Rosemberg, C., Alam, F., Warlaumont, A. S., \& Bunce, J. (2021). Developing a Cross-Cultural Annotation System and MetaCorpus for Studying Infants' Real World Language Experience. \emph{Collabra: Psychology}, \emph{7}(1), 23445. \url{https://doi.org/10.1525/collabra.23445}

\leavevmode\hypertarget{ref-yoshinaga-itano2020}{}%
Yoshinaga-Itano, C., Sedey, A. L., Mason, C. A., Wiggin, M., \& Chung, W. (2020). Early Intervention, Parent Talk, and Pragmatic Language in Children With Hearing Loss. \emph{Pediatrics}, \emph{146}(Supplement\_3), S270--S277. \url{https://doi.org/10.1542/peds.2020-0242F}

\leavevmode\hypertarget{ref-yurovsky2016}{}%
Yurovsky, D., Doyle, G., \& Frank, M. C. (2016). Linguistic input is tuned to children's developmental level. \emph{Cognitive Science}, 6.


\end{document}
