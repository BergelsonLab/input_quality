---
title             : "Comparing Language Input in Homes of Young Blind and Sighted Children: Insights from Daylong Recordings"

shorttitle        : "Language Input to Blind and Sighted Children"

author:
  - name          : "Erin Campbell"
    affiliation   : "1,2"
    corresponding : yes    
    email         : "eecamp@bu.edu"
    address       : "2 Silber Way, Room 513, Boston, MA 02215"
  - name          : "Lillianna Righter"
    affiliation   : "1,3"
  - name          : "Eugenia Lukin"
    affiliation   : "1,3"
  - name          : "Elika Bergelson"
    affiliation   : "1,3"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology & Neuroscience, Duke University, Durham, NC"
  - id            : "2"
    institution   : "Wheelock College of Education & Human Development, Boston University, Boston, MA"
  - id            : "3"
    institution   : "Department of Psychology, Harvard University, Cambridge, MA"

note: |
  **Conflicts of Interest**: The authors have no conflicts of interest to report.
  **Funding**: This work was supported by the National Science Foundation CAREER grant (BCS-1844710) to EB and Graduate Research Fellowship (2019274952) to EC.
  

keywords          : "language input; blind children; LENA"

bibliography      : ["VI_input_citations.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
header-includes:
  - \usepackage{float}

---

```{r preferences, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, out.extra = "")

set.seed(553)

preprocess <- FALSE
new_graphs <- FALSE
```

```{r libraries}
library(tidyverse)
library(ggplot2)
library(plotrix)
library(see)
library(papaja)
library(stringr)
library(reshape2)
library(lubridate)
library(corrplot)
library(psych)
library(glue)
library(knitr)
library(kableExtra)
library(irr)
library(udpipe)
library(cowplot)
library(ggpubr)
```

```{r loading-data}
if (preprocess == TRUE) {source("./manuscript/input_quality_preprocessing.R")} else {

# praise: zhenya2erin: The READMEs in the data folders are very informative!

# quantity
LENA_counts <- read_csv("./data/LENA/Automated/LENA_counts.csv")
manual_word_tokens <- read_csv("./data/LENA/Transcripts/Derived/manual_word_tokens.csv")
VITD_transcripts <- read_csv("./data/LENA/Transcripts/Derived/VITD_transcripts.csv")
VITD_LENA_words <- read_csv("./data/LENA/Transcripts/Derived/VITD_LENA_words.csv")
# interactive
xds_props <- read_csv("./data/LENA/Transcripts/Derived/xds_props.csv")
xds_props_wide <- read_csv("./data/LENA/Transcripts/Derived/xds_props_wide.csv")
# linguistic
MLUs <- read_csv("./data/LENA/Transcripts/Derived/MLUs.csv")
MLU_subset_for_agreement <- read_csv("./data/LENA/Transcripts/Derived/MLU_subset_for_agreement.csv")
TTR_calculations <- read_csv("./data/LENA/Transcripts/Derived/TTR_calculations.csv")
raw_TTR_calculations <- read_csv("./data/LENA/Transcripts/Derived/TTR_calculations.csv")
# conceptual
sensory_props <- read_csv("./data/LENA/Transcripts/Derived/sensory_props.csv")
sensory_props_wide <- read_csv("./data/LENA/Transcripts/Derived/sensory_props_wide.csv")
temporality_props <- read_csv("./data/LENA/Transcripts/Derived/temporality_props.csv")
temporality_props_wide <- read_csv("./data/LENA/Transcripts/Derived/temporality_props_wide.csv")
content_words_only <- read_csv("./data/LENA/Transcripts/Derived/content_words_only.csv")
verbs_only <- read_csv("./data/LENA/Transcripts/Derived/verbs_only.csv")
displacement_subset_for_agreement <- read_csv("./data/LENA/Transcripts/Derived/displacement_subset_for_agreement.csv")

perc_random_silent <- read_rds("./data/LENA/Transcripts/Derived/perc_random_silent.rds")
percentage_not_na_sensory <- read_rds("./data/LENA/Transcripts/Derived/percentage_not_na_sensory.rds")

}


#demo
VI_matches_demo <- read_csv("./data/Demographics/VI_matches_demo.csv", na = c("", "NA", " "))


```

```{r functions}
report_continuous <- function(vector, string,dig=1){
  range <- glue("{round(min(vector,na.rm=TRUE))}--{round(max(vector,na.rm=TRUE))}")
  mean <- glue("{round(mean(vector,na.rm=TRUE),dig)}")
  sd = glue("{round(sd(vector,na.rm=TRUE),dig)}")
  template <- "{range}{string},\n{mean} ({sd}){string}"
  glue(template, range = range, mean = mean, sd = sd)
}

level_template <- "{level}: {pct}%"
report_level <- function(vector, level) {
  fac = as.factor(vector)
  n <- sum(fac == level, na.rm = TRUE)
  total <- length(fac)
  pct <- round(n / total * 100)
  glue(level_template, n = n, pct = pct, level = level)
}
report_factor <- function(vector) {
  levels(as.factor(vector)) %>%
    map_chr(function(level) report_level(vector = vector, level = level)) %>%
    str_c(collapse = ',\n')
}

get_group_means <- function(data, column, subset, unit = "",adjust=1,digits=2) {
  filtered_data <- data %>%
    filter({{ subset }})
  mean_value <- mean(filtered_data[[column]], na.rm=TRUE)*adjust
  median_value <- median(filtered_data[[column]], na.rm=TRUE)*adjust
  min_value <- min(filtered_data[[column]], na.rm=TRUE)*adjust
  max_value <- max(filtered_data[[column]], na.rm=TRUE)*adjust
  values_string <- glue::glue("{round(mean_value,digits)}, {round(median_value,digits)}, {round(min_value,digits)}-{round(max_value,digits)} {unit}")
  return(values_string)
}
#function to convert things into percents
percent <- function(x, digits = 1, format = "f", ...) {      # Create user-defined function
  paste0(formatC(x * 100, format = format, digits = digits, ...), "%")
}
seconds_to_length <- function(seconds, format = "f", ...) {      
  paste0(hour(seconds_to_period(seconds)), " hours ", minute(seconds_to_period(seconds)), " minutes")
}

make_blind_sighted_violins <- function(data, x_var, y_var, group_var, y_label, title = NULL, y_axis_start=0) {
  ggplot(data = data, aes_string(x = x_var, y = y_var, color = group_var, fill = group_var)) +
    geom_violinhalf(alpha = .5, trim = FALSE, flip=1) +
    geom_line(aes_string(group = "pair"), color = "grey10", alpha = .1) +
    theme_classic() +
    labs(y = y_label, x = NULL, title = title, element_text(size = 10)) +
    theme(
      legend.title = element_blank(),
      legend.position = "none",
      axis.text.x = element_text(angle = 45, hjust = 1),
      text = element_text(size = 10)
    ) +
    scale_color_manual(
      breaks = c("TD", "VI"),
      labels = c("Sighted", "Blind"),
      values = c("#b16fff", "#ade7f5")
    ) +
    scale_fill_manual(
      breaks = c("TD", "VI"),
      labels = c("Sighted", "Blind"),
      values = c("#b16fff", "#ade7f5")
    ) +
    stat_summary(fun.data = "mean_cl_boot", color = 'black') +
    scale_x_discrete(labels = c("VI" = "Blind", "TD" = "Sighted")) +
    coord_cartesian(ylim = c(y_axis_start, max(data[[y_var]] * 1.1))) +
    ggpubr::stat_compare_means(label =  "p.signif", paired=TRUE, label.x = 1.5)
}

cowplot_title <- function(title_text) {
  title <- ggdraw() + 
    draw_label(
      title_text,
      fontface = 'bold',
      x = 0,
      hjust = 0
    ) +
    theme(
      plot.margin = margin(0, 0, 0, 7)
    )
  return(title)
}

```

# Abstract

We compared everyday language input to young congenitally-blind children with no additional disabilities (N=15, 6--30mo., M:16mo.) and demographically-matched sighted peers (N=15, 6--31mo., M:16mo.). By studying whether the language input of blind children differs from their sighted peers, we aimed to determine whether, in principle, the language acquisition patterns observed in blind and sighted children could be explained by aspects of the speech they hear. Children wore LENA recorders to capture the auditory language environment in their homes. Speech in these recordings was then analyzed with a mix of automated and manually-transcribed measures across various subsets and dimensions of language input. These included measures of quantity (adult words), interaction (conversational turns and child-directed speech), linguistic properties (lexical diversity and mean length of utterance), and conceptual features (talk centered around the here-and-now; talk focused on visual referents that would be inaccessible to the blind but not sighted children). Overall, we found broad similarity across groups in speech quantity, interaction, and linguistic properties. The only exception was that blind children's language environments contained slightly but significantly more talk about past/future/hypothetical events than sighted children's input; both groups received equivalent quantities of "visual" speech input. The findings challenge the notion that blind children's language input diverges substantially from sighted children's; while the input is highly variable across children, it is not systematically so across groups, across nearly all measures. The findings suggest instead that blind children and sighted children alike receive input that readily supports their language development, with open questions remaining regarding how this input may be differentially leveraged by language learners in early childhood.

# Introduction

The early language skills of blind children are highly variable. Some children demonstrate age-appropriate vocabulary and grammar from the earliest stages of language learning, while others experience substantial language delays [@landau1985; @bigelow1987; @campbell2024]. By adulthood, however, blind individuals are fluent language-users, even demonstrating faster lexical processing skills than sighted adults [@roder2000; @roder2003; @loiotile2020; though c.f., @sak-wernicka2017 for discussion of possible pragmatic differences]. The causes of early variability and the potential ability (or need) to "catch up" remain poorly understood: what could make the language learning problem different or initially more difficult for the blind child? Here, we compare the language environments of blind children to that of their sighted peers. In doing so, we begin to untangle the role that perceptual input plays in shaping children's language environment, and better understand the interlocking factors that may contribute to variability in blind children's early language abilities.

## Why would input matter?

Among both typically-developing children and children with developmental differences, language input has been found to predict variability in language outcomes [@gilkerson2018; @huttenlocher1991; @rowe2008; @anderson2021; @rowe2012; @anderson2021; @huttenlocher2010]. The many ways to operationalize language input tend to be grouped into **quantity** measures and **input characteristics** (often referred to as "quality" measures)[^1]. Quantity can be broadly construed as the number of words or utterances a child is exposed to. At a coarse level, children who are exposed to more speech [or sign, @watkins1998] tend to have stronger language outcomes and produce more speech themselves [@gilkerson2018; @huttenlocher1991; @rowe2008; @anderson2021; @bergelson2023].

[^1]: We avoid the term "quality" here as it carries potential biases regarding linguistic norms (MacLeod & Demers, 2023).

Previous research suggests that the specific characteristics of language input are perhaps even more influential than quantity measures alone [@rowe2012; @hirsh-pasek2015], although they are somewhat trickier to delineate and assess. Rowe and Snow (2020) categorized this space into three dimensions: interactive features (e.g., parent responsiveness, speech directed *to* child vs. overheard, conversational turn-taking), linguistic features (e.g., lexical diversity, grammatical complexity), and conceptual features (i.e., the extent to which input focuses on the *here-and-now*), which we adopt here.

In terms of interactive features, previous studies have indicated that back-and-forth communicative exchanges (also known as conversational turns) between caregivers and children are predictive of better language outcomes across infancy [@goldstein2008; @donnellan2020] and toddlerhood [@hirsh-pasek2015; @romeo2018]. Another way to quantify the extent to which caregivers and infants interact is by looking at how much speech is directed *to* the child (as opposed to, for example, an overheard conversation between adults). The amount of child-directed speech in children's input [at least in Western contexts, @casillas2020] has been linked to children's vocabulary size and lexical processing [@weisleder2013; @rowe2008; @shneidman2013].

Under the linguistic umbrella, we can measure the *kinds* of words used (often measured as lexical diversity, type-token ratio), and the ways they are *combined* (syntactic complexity, often measured by mean length of utterance). Both parameters have been found to correlate with children's language growth: sighted toddlers who are exposed to a greater diversity of words in their language input are reported to have larger vocabulary scores [@rowe2012; @anderson2021; @huttenlocher2010; @hsu2017; @weizman2001]. Likewise, the diversity and complexity of syntactic constructions in parental language input has been associated with both children's vocabulary growth and structural diversity in their own productions [@hoff2003; @naigles1998; @devilliers1985; @huttenlocher2010; @hadley2017; @huttenlocher2002].

Finally, the conceptual dimension of language input aims to capture the extent to which the language signal maps onto present objects and ongoing events in children's environments [@rowe2020]. As children develop, their ability to represent abstract referents improves [@luchkina2020; @kramer1975; @bergelson2013]. Decontextualized language input-- that is, talking about past, future, or hypothetical events, or people and items that are not currently present in the environment-- may be one contributing factor [@rowe2013]. Greater prevalence of decontextualized language in input to toddlers has been found to predict aspects of children's own language in kindergarten and beyond [@rowe2012; @demir2015; @uccelli2019].

From this (necessarily abridged) review, it appears that many factors in the language input alone link to how sighted children learn about the world and language, but that children also learn from sensory, conceptual, and social knowledge. Many cues for word learning are visual: for example, empirical work finds that sighted children can leverage visual information like parental gaze, shared visual attention [@tomasello1986], pointing [@lucca2018], and the presence of salient objects in the visual field [@yu2012]. Because these visual cues are inaccessible to blind children, language input may take on a larger role in the discovery of word meaning [@campbell2022]. Syntactic structure in particular provides critical cues to word meaning, such as the relationship between two entities that aren't within reach, or are intrinsically unobservable or ambiguous [@gleitman1990]. But in order to evaluate whether language input plays a larger role for blind versus sighted children's learning, it is worth first establishing whether blind and sighted children's language input differs. That is, children with different sensory access could differentially make use of the same kind of language input, or they could apply the same learning mechanisms to input with different properties--a debate carried over from work with typically-sighted children [@newport1977]. Either way, characterizing the input across potentially relevant dimensions is a helpful first step.

## Why would the input differ between blind and sighted children?

Speakers regularly tailor their speech to communicate efficiently with the listener [@grice1975]. Across many contexts, research finds that parents are sensitive to their child's developmental level and tune language input accordingly [@snow1972; @vygotsky1978; @newport1977]. One example is child-directed speech, wherein parents speak to young children with exaggerated prosody and slower speech [@fernald1989; @bernsteinratner1984; @moser2022; @newport1977], which are in some cases helpful to the young language learner [@thiessen2005]. For instance, parents tend to repeat words more often when interacting with infants than with older children or adults [@snow1972; @fernald1993]. Communicative tailoring is also common in language input to children with disabilities, who have been found to receive simplified, more directive language input, and less interactive input compared to typically-developing children [@dirks2020; @yoshinaga-itano2020]. In other contexts, language input to children with disabilities has been shown to be more multimodal, such that parents more frequently combine communicative cues [e.g., speech and touch, @abu-zhaya2019] when interacting with deaf children, compared to their typically-hearing peers.

In addition to tailoring communication to children's developmental level, speakers also adjust their conversation in accordance to their conversational partner's sensory access [@gergle2004; @grigoroglou2016]. In a noisy environment, speakers often adapt the acoustic-phonetic features of their speech to make it easier for their interlocutor to understand them [@hazan2011], demonstrating sensitivity to even temporary sensory conditions. When describing scenes, speakers tend to provide the information their listeners lack but avoid redundant visual description [@ostarek2019; @grice1975]. During in-lab tasks with sighted participants, participants in several studies verbally provide visually-absent cues when an object is occluded to their partner [@jara-ettinger2021; @hawkins2021; @rubio-fernandez2019]. These results suggest that adults and even infants [@senju2013; @ganea2018; @chiesa2015] can flexibly adapt communication to the visual and auditory abilities of their partner.

Taking these results into consideration, we might expect parents of blind children to verbally compensate for missing visual input, perhaps providing more description of the child's environment. But prior research doesn't yield a clear answer. Several studies suggest differences in the conceptual features: caregivers of blind children restrict conversation to things that the blind child is currently engaged with, rather than attempt to redirect their attention to other stimuli [@andersen1993; @campbell2003; @kekelis1984; though c.f., @moore1994]. Studies of naturalistic input to blind children report that parents use *fewer* declaratives and *more* imperatives than parents of sighted children, suggesting that blind children might be receiving less description than sighted children [@kekelis1984; @landau1985; though c.f., @lukin2023; @perez-pereira2001]. Other studies report that parents adapt their interactions to their children's visual abilities, albeit in specific contexts. @tadic2013a find that in a structured book-reading task, parents of blind children provide more descriptive utterances than parents of sighted children. Further, parents of blind children have been found to provide more tactile cues to initiate interactions or establish joint attention [@urwin1983; @urwin1984; @preisler1991], which may serve the same social role as shared gaze in sighted children. These mixed results suggest that parents of blind children might alter language input in some domains but not others. The apparent conflict in results may be exacerbated by the difficulty of recruiting specialized populations to participate in research: the small (in most cases, single-digit) sample sizes of prior work limits our ability to generalize about any differences in the input to blind vs. sighted infants.

## The Present Study

Children can and do learn language in a variety of input scenarios [@gleitman1995], but if language input differs systematically between blind infants and toddlers, capturing this variation may reveal a more nuanced picture of how infants use the input to learn language. In the present study, we examine daylong recordings of the naturalistic language environments of blind and sighted children in order to characterize the input to each group. Using both automated measures and manual transcription of these recordings, we measure input quantity (adult word count) and analyze several characteristics that have been previously suggested to be information-rich learning cues, including interaction (conversational turn count, proportion of child-directed speech), conceptual features (temporal displacement, sensory modality), and linguistic complexity (type-token ratio and mean length of utterance). Though the present study is largely exploratory, based on prior research [i.e., @andersen1993; @rowland1984; @perez-pereira2001; @moore1994; @kekelis1984; @preisler1991; @grumi2021; @campbell2003; @kekelis1984; @lorang2020; @dirks2020], we predict that blind vs. sighted children would have input featuring less interactivity (fewer conversational turns and less child-directed speech), less linguistic complexity (lower type-token ratio and shorter utterances), and conceptual content focused more on the child's locus of attention (more here-and-now speech and fewer visual words); we have no *a priori* hypotheses regarding language input quantity.

# Methods

```{r participant-characteristics, results="asis"}
# ec2eb: yes! checked, and that is indeed what *we* have access to, but I will do some more digging and see if I can get that info from more of our collaborators//eb2ec: please do!
participants_summary <- LENA_counts %>%
  mutate(Age_days = as.numeric(str_sub(VIHI_ID, 8, length(VIHI_ID))),
         Age_months = Age_days * 0.0328767) %>%
  group_by(group) %>%
  summarize(age = mean(Age_months,na.rm=TRUE),
            min_age = min(Age_months),
          max_age = max(Age_months),
          N=n())

VI_matches_demo_clean <- VI_matches_demo %>%
  mutate(match_group = as.factor(case_when(
    match_group == "VI_TD" ~ "Sighted (N=15)",
    match_group == "VI" ~ "Blind (N=15)")
  ),
  Race = if_else(Race == "Black or African American,White", "Mixed", Race),
  MaternalEd = as.factor(case_when(
    MaternalEd == "Doctoral degree" ~ "Graduate degree",
    MaternalEd == "Master's degree" ~ "Graduate degree",
    TRUE ~ MaternalEd
  )),
MaternalEd = factor(MaternalEd, levels = c("Some college", "Associate's degree",
 "Bachelor's degree", "Graduate degree"
 )),
  Gender = recode(Gender, "F" = "Female", "M" = "Male"))  %>%
  dplyr::rename(Group = match_group) %>%
  mutate(
    across(c(Gender,Race), ~replace_na(.x, "Missing")),
           Gender = factor(Gender,levels=c("Female","Male")),
) 

VI_matches_demo_blind <- VI_matches_demo_clean %>%
  filter(Group == "Blind (N=15)")

VI_matches_demo_sighted <- VI_matches_demo_clean %>%
  filter(Group == "Sighted (N=15)")

as.data.frame(
  tribble(
    ~ Variable,
    ~ "Blind (N=15)",
    ~ "Sighted (N=15)",
    "Age (months)",
    report_continuous(VI_matches_demo_blind$Age_months, ""),
    report_continuous(VI_matches_demo_sighted$Age_months, ""),
    "Sex",
    report_level(VI_matches_demo_blind$Gender, "Female"),
    report_level(VI_matches_demo_sighted$Gender, "Female"),
    "",
    report_level(VI_matches_demo_blind$Gender, "Male"),
    report_level(VI_matches_demo_sighted$Gender, "Male"),
    "Number of Older Siblings",
    report_continuous(VI_matches_demo_blind$OlderSiblings, ""),
    report_continuous(VI_matches_demo_sighted$OlderSiblings, ""),
    "Race",
    report_level(VI_matches_demo_blind$Race, "American Indian or Alaska Native"),
    report_level(VI_matches_demo_sighted$Race, "American Indian or Alaska Native"),
    "",
    report_level(VI_matches_demo_blind$Race, "Black or African American"),
    report_level(VI_matches_demo_sighted$Race, "Black or African American"),
    "",
    report_level(VI_matches_demo_blind$Race, "Mixed"),
    report_level(VI_matches_demo_sighted$Race, "Mixed"),
    "",
    report_level(VI_matches_demo_blind$Race, "White"),
    report_level(VI_matches_demo_sighted$Race, "White"),
    "",
    report_level(VI_matches_demo_blind$Race, "unknown"),
    report_level(VI_matches_demo_sighted$Race, "unknown"),
    "Maternal Education",
    report_level(VI_matches_demo_blind$MaternalEd, "Some college"),
    report_level(VI_matches_demo_sighted$MaternalEd, "Some college"),
    "",
    report_level(VI_matches_demo_blind$MaternalEd, "Associate's degree"),
    report_level(VI_matches_demo_sighted$MaternalEd, "Associate's degree"),
    "",
    report_level(VI_matches_demo_blind$MaternalEd, "Bachelor's degree"),
    report_level(VI_matches_demo_sighted$MaternalEd, "Bachelor's degree"),
    "",
    report_level(VI_matches_demo_blind$MaternalEd, "Graduate degree"),
    report_level(VI_matches_demo_sighted$MaternalEd, "Graduate degree"),
    "Ethnicity",
    report_level(VI_matches_demo_blind$Ethnicity, "Hispanic or Latino"),
    report_level(VI_matches_demo_sighted$Ethnicity, "Hispanic or Latino"),
    "",
    report_level(VI_matches_demo_blind$Ethnicity, "Not Hispanic or Latino"),
    report_level(VI_matches_demo_sighted$Ethnicity, "Not Hispanic or Latino"),
    "",
    report_level(VI_matches_demo_blind$Ethnicity, "unknown"),
    report_level(VI_matches_demo_sighted$Ethnicity, "unknown"),
  )
)  %>%
  kable(caption = "Demographic characteristics of the blind and sighted samples. For continuous variables, range and mean are provided. For categorical variables, percentages by level are provided.") %>%
  kable_styling(font_size = 8)
```

## Participants

This study included `r LENA_counts %>% filter(group=="VI") %>% distinct(VIHI_ID, .keep_all =TRUE) %>% nrow()` congenitally-blind infants and their families[^2]. To be eligible, participants had to be 6--30 months old, have severe to profound visual impairment (i.e. at most light perception), no additional disabilities (developmental delays, intellectual disabilities, or hearing loss), and be exposed to $\geq$ 75% English at home. Blind participants were recruited through ophthalmologist referral, preschools, early intervention programs, social media, and word of mouth. Blindness in our sample was caused by a range of conditions, including cataracts (n=`r VI_matches_demo %>% filter(Diagnosis == "Cataracts") %>% nrow()`), Leber's Congenital Amaurosis (n=`r VI_matches_demo %>% filter(Diagnosis == "Leber's Congenital Amaurosis") %>% nrow()`), Microphthalmia (n=`r VI_matches_demo %>% filter(Diagnosis == "Microphthalmia") %>% nrow()`), Ocular albinism (n=`r VI_matches_demo %>% filter(Diagnosis == "Ocular albinism") %>% nrow()`), Optic Nerve Hypoplasia (n=`r VI_matches_demo %>% filter(Diagnosis == "Optic Nerve Hypoplasia") %>% nrow()`), Retinal Detachments (n=`r VI_matches_demo %>% filter(Diagnosis == "Retinal Detachments") %>% nrow()`), and Retinopathy of Prematurity (n=`r VI_matches_demo %>% filter(Diagnosis == "Retinopathy of Prematurity") %>% nrow()`). Etiology was unknown in `r VI_matches_demo %>% filter(Diagnosis == "Not specified") %>% nrow()` participants, and `r VI_matches_demo %>% filter(Diagnosis == "Multiple") %>% nrow()` participants had multiple contributing conditions. Caregivers were also asked to complete a demographics survey and the MacArthur-Bates Communicative Development Inventory [CDI, @fenson1994] within one week of the home language recording.

[^2]: One family contributed two recordings for the same blind child. In the present study, we used only the first recording from that participant.

To control for the wide age range of the study, each blind participant was matched to a sighted participant, based on age ($\pm$ 6 weeks), sex, maternal education ($\pm$ one education level), and number of siblings ($\pm$ 1 sibling). Sighted matches were drawn from the multiple existing corpora [@vandam2015; @vandam2016; @bergelson2015a; @bergelson2019; @ramirez-esparza2014; @warlaumont2016; @wang2022; @rowland2018], or when there was no recording available that matched a blind participant's demographic characteristics, collected *de novo*. See Table \@ref(tab:participant-characteristics) for sample demographic characteristics.

## Recording Procedure

For the recording portion of the study, caregivers of participating infants received a LENA wearable audio recorder and vest [@ganek2016; @gilkerson2008]. They were instructed to place the recorder in the vest on the day of their scheduled recording and put the vest on their child from the time they woke up until the recorder automatically shut off after 16 hours (setting the vest nearby during baths, naps, and car rides). Actual recording length ranged from `r seconds_to_length(min(LENA_counts$total_time_dur))` to `r seconds_to_length(max(LENA_counts$total_time_dur))` (Mean: `r seconds_to_length(mean(LENA_counts$total_time_dur, na.rm=TRUE))`).

## Processing

The audio recordings were first processed by the LENA proprietary software [@xu2009], creating algorithmic measures such as conversational turn count and adult word count. Each recording was then run through an in-house automated sampler that selected 15- non-overlapping 5-minute segments, randomly distributed across the duration of the recording. Each segment consists of 2 core minutes of annotated time, with 2 minutes of listenable context preceding the annotation clip and 1 minute of additional context following. Because these segments were sampled randomly, across participants roughly `r round(perc_random_silent)`% of the random 2-minute coding segments contained no speech at all. For questions of *how much does a phenomenon occur*, random sampling schemes can help avoid overestimating speech in the input, but for questions of input *content*, randomly selected samples may be too sparse [@pisani2021].

Therefore, we chose to annotate 5 additional (non-overlapping) 2-minute segments specifically for their high density of speech. To select these segments of dense talk, we first conducted an automated analysis of the audio file using the voice type classifier for child-centered daylong recordings [@lavechin2021] which identified segments likely containing human speech. The entire recording was divided into 2-minute chunks, each ranked highest to lowest by the total duration of the speech segments contained within the chunk. We annotated the 5 highest-ranked segments of each recording. These high-volubility segments allow us to more closely compare our findings to studies classifying the input during structured play sessions, which paint a denser and differently-proportioned makeup of the language input [@bergelson2019]. In sum, 30 minutes of randomly-sampled input and 10 minutes of high-volubility input (40 minutes total) were annotated per child.

## Annotation

Manual annotation of the selected segments was conducted using the ELAN software [@brugman2009]. Trained annotators listened through each 2-minute segment plus its surrounding context and coded it using the ACLEW annotation scheme [@soderstrom2021]. For more information about this scheme, see the [ACLEW homepage](https://sites.google.com/view/aclewdid/home "ACLEW homepage"). Speech by people other than the target child was transcribed using an adapted version of the CHAT transcription style [@macwhinney2019; @soderstrom2021]. Because the majority of target children in the project are pre-lexical, utterances (e.g. babble) produced by the target child are not yet transcribed. Speech was then further classified by the addressee of each utterance: child, adult, both an adult and a child, pets or other animals, unclear addressee, or a recipient that doesn't fit into another category (e.g., voice control of Siri or Alexa, prayer to a metaphysical entity).

### Manual Annotation Training and Reliability

All annotators are tested on the ACLEW scheme prior to beginning corpus annotation, until they reach 95% agreement or better with a "gold standard" coder for segmentation and utterance classification. Training often takes upwards of 20 hours of annotation practice. Following the first pass by annotators, all files were reviewed by a highly-trained "superchecker" to ensure consistency between coders and check for errors. Ten percent of clips were re-transcribed to assess reliability; further reliability data are provided in corresponding sections below.

## Extracting Measures of Language Input

To go from our dimensions of interest (quantity, interactiveness, linguistic, conceptual), to quantifiable properties, we used a combination of automated measures [generated by the proprietary LENA algorithm, @xu2009] and manual measures (generated from the transcriptions and classifications made by our trained annotators). Altogether, this corpus presently includes approximately `r round(sum(LENA_counts$total_time_dur) / 60 / 60)` hours of audio, `r nrow(VITD_transcripts)` utterances, and `r nrow(VITD_LENA_words)` words. LENA measures were calculated over the whole day, and then normalized by recording length. Transcription-based quantity and interactiveness analyses were conducted on the random samples only, to capture a more representative estimate. Linguistic and conceptual analyses were conducted on all available annotations in order to maximize the amount of speech over which we could calculate them. These measures are described below and summarized in Table \@ref(tab:analysis-summary-table).

### Quantity

#### Automated Word Count

To derive this count, the LENA algorithm segments the recording into clips which are then classified by speaker's perceived gender (male/female), age (child/adult), and distance (near/far), as well as several non-human speaker categories (e.g., silence, electronic noise). Only segments that are classified as nearby male or female adult speech are then used by the algorithm for its subsequent Adult Word Count (AWC) estimation [@xu2009]. Validation work suggests that this automated count correlates strongly with word counts derived from manual annotations [r = .71 -- .92, @lehet2021; @cristia2020], and meta-analytic work finds that AWC is associated with children's language outcomes across developmental contexts [e.g., autism, hearing loss, @wang2020]. Because the recordings varied in length (`r seconds_to_length(min(LENA_counts$total_time_dur))` to `r seconds_to_length(max(LENA_counts$total_time_dur))`), we normalized AWC by dividing by recording length[^3].

[^3]: To make these measures more comparable, we present both in terms of words per hour.

#### Manual Word Count

We also calculated a manual count of speech in the children's environment. Manual Word Count (MWC) is simply the number of intelligible words in our transcriptions of each child's recording. Speech that was too far or muffled to be intelligible, as well as speech from the target child and electronic speech (TV, radio, toys) are excluded from this count. Unlike LENA's AWC, MWC contains speech from other speakers in the child's environment (e.g., siblings), not just from adults.

By using automated *and* manual measures of quantity, we hope to capture complementary estimates of the amount of speech children are exposed to. While AWC is considered less accurate than manual annotation, it is commonly used due to its ability to readily provide an estimate of the adult speech across the whole day. MWC, because it comes from human annotations, is the gold-standard for accurate speech estimates, but due to feasibility, is only derived from 30 minutes of the recording (sampled in 2-minute clips, at random, as described above).

### Interaction

#### Conversational Turn Count

One common metric of communicative interaction [e.g., @ganek2018; @magimairaj2022] is conversational turn count (or CTC), an automated measure generated by LENA [@xu2009]. Like AWC, a recent meta-analysis finds that CTC is associated with children's language outcomes [@wang2020]. After tagging vocalizations for speaker identity, the LENA algorithm looks for alternations between adult and target child speech in close temporal proximity (within 5 seconds). This can erroneously include non-contingent interactions (e.g., mom talking to dad while the infant babbles to herself nearby), and therefore inflate the count especially for younger ages and in houses with multiple children [@ferjanramirez2021]. Still, this measure correlates moderately well with manually-coded conversational turns [@busch2018; @ganek2018], and because participants in our sample are matched on both age and number of siblings, CTC overestimation should not be biased towards either group.

#### Proportion of Child-Directed Speech

Our other measure of interaction is the proportion of utterances that are child-directed, derived from the manual annotations. Each proportion was calculated as the number of utterances (produced by someone *other* than the target child) tagged with a child as the addressee, out of the total number of utterances. Annotator agreement for addressee was 93%, with a kappa of 0.90 [CI: 0.89--0.91].

### Linguistic Features

#### Type-Token Ratio

As in previous work [e.g., @montag2018; @templin1957; @pancsofar2006], we calculated the lexical diversity of the input by dividing the number of unique words by the total number of words (i.e., the type-token ratio). Because the type-token ratio changes as a function of the number of words in a sample [@richards1987; @montag2018], we first standardized the size of the sample by cutting the manual annotations in each recording into 100-word bins. We then calculated the type-token ratio within each of these bins by dividing the number of unique words in each bin by the number of total words (\~100) and then averaged the type-token ratio across bins for each child[^4]. This provided a measure of lexical diversity: per 100 words, how many unique words are children exposed to?

[^4]: Computing TTR over the entire sample instead of averaging over 100-word bins rendered the same pattern of results.

```{r agreement}
# Calculate Intraclass Correlation
MLU_agreement <- icc(MLU_subset_for_agreement %>%
                       dplyr::select(manual_morpheme_count, morphemecount))
displacement_agreement <-
  sum(
    displacement_subset_for_agreement$temporality == displacement_subset_for_agreement$manual_temporality,
    na.rm = TRUE
  ) / nrow(displacement_subset_for_agreement %>% filter(!is.na(manual_temporality)))
displacement_confusion_matrix <- table(displacement_subset_for_agreement$temporality,
       displacement_subset_for_agreement$manual_temporality)
displacement_kappa <- cohen.kappa(displacement_confusion_matrix)

n_uncat<-sum(verbs_only$temporality == "ambiguous")

```

#### MLU

We also analyzed the syntactic complexity of children's language input, approximated as mean utterance length in morphemes. Each utterance in a child's input was tokenized into morphemes using the 'morphemepiece' R package [@bratt2022]. We then calculated the mean length of utterance (number of morphemes) in each audio recording. We manually checked utterance length in a random subset of 10% of the utterances (n = `r nrow(MLU_subset_for_agreement)` utterances), which yielded a intra-class correlation coefficient of `r MLU_agreement$value` agreement with the morphemepiece approach (CI: `r printnum(MLU_agreement$lbound)`--`r printnum(MLU_agreement$ubound)`, *p* `r printp(MLU_agreement$p.value)`), indicating high consistency.

### Conceptual Features

Our analysis of the conceptual features aims to measure whether the extent to which language input centers around the *"here and now"*: things that are currently present or occurring that a child may attend to in real time. We approximate *here-and-now*ness using lexical and morphosyntactic properties of the input.

#### Proportion of temporally displaced verbs

We examined the displacement of events discussed in children's linguistic environment, via properties of the verbs in their input. Notably, we are attempting to highlight semantic features of the language environment. We do so here by categorizing utterances based on the syntactic and morphological features of verbs, since these contain some time information in their surface forms. We assigned each utterance a temporality value: utterances tagged "displaced" describe events that take place in the past, future, or irrealis space, while utterances tagged "present" describe current, ongoing events. This coding scheme roughly aligns with both the temporal displacement and future hypothetical categories in [@grimminger2020; see also: @hudson2002; @lucariello1987].

To do this, we used the udpipe package [@wijffels2023] to tag the transcriptions with parts of speech and other lexical features, such as tense, number agreement, or case inflection. To be marked as present, a verb either had to be marked with both present tense and indicative mood, or appear in the gerund form with no marked tense (e.g. 'you talking to Papa?'). Features that could mark an utterance as displaced included past tense, presence of a modal, presence of 'if', or presence of 'gonna'/'going to', 'have to', 'wanna'/'want to', or 'gotta'/'got to', since these typically indicate future events, belief states and desires, rather than real-time events. In the case of utterances with multiple verbs, we selected the features from the first verb or auxiliary, as a proxy for hierarchical dominance. Utterances without verbs were excluded. A small number of verb-containing utterances in our corpus were left "ambiguous" (n = `r n_uncat`/`r nrow(verbs_only)`), either because they were fragments or because the automated parser failed to tag any of the relevant features. We manually checked verb temporality in a random subset of 10% of the utterances (n = `r nrow(displacement_subset_for_agreement)`); human judgments of event temporality aligned with the automated tense tagger `r round(displacement_agreement,2) * 100`% of the time (Kappa = `r printnum(displacement_kappa[["weighted.kappa"]])`, CI:`r printnum(displacement_kappa$confid[[4]])`-`r printnum(displacement_kappa$confid[[6]])`, *p* = `r printp(displacement_kappa$plevel)`), indicating substantial agreement, with the majority of discrepancies occurring on words the tagger categorized as ambiguous.

#### Proportion of highly visual words

In addition to this general measure of decontextualized language, we include one measure that is uniquely decontextualized for blind children: the proportion of words in the input with referents that are highly and exclusively visual. We first filter the input to only content words (excluding, for example: *the*, *at*, *of*). We then categorize the perceptual modalities of words' referents using the Lancaster Sensorimotor Norms, which are ratings from sighted adults noting the extent to which a word evokes a word evokes a sensory experience in a given modality [@lynott2020]. Each of the approximately 40,000 words in the Lancaster Sensorimotor Norms gets a score for each of 6 sensory modalities (auditory, haptic, gustatory, interoceptive, olfactory, visual). In this rating system, words with higher ratings in a given modality are more strongly associated with perceptual experience in that modality, and a word's dominant perceptual modality is the modality which received the highest mean rating. We tweak this categorization in two ways: we categorized content words that received relatively low ratings across all modalities (\<3.5./5) as predominantly *amodal*, and content words whose ratings were distributed across modalities were categorized as *multimodal*[^5]. Using this system, each of the content words in children's input were categorized into their primary perceptual modality; `r percentage_not_na_sensory`% of the words in our corpus had a corresponding word in the Lancaster ratings and could be categorized in this way. For each child, we extracted the proportion of exclusively "visual" words in their home speech sample.

[^5]: Words with perceptual exclusivity scores \< 0.5 [calculated as a word's range of ratings across modalities divided by the sum of ratings across modalities, @lynott2020] were re-categorized as multimodal. The cut-offs for classifying amodal and multimodal words were chosen based on authors' intuitions regarding what thresholds seemed to classify the words well into amodal, multimodal, and visual phenomena. That said, results are robust across a range of thresholds, and all data are provided to interested readers should they be interested in considering other values.

# Results

## Measuring Properties of Language Input

Our study assesses whether language input to blind children is different from the language input to sighted children, along the dimensions of quantity, interaction, linguistic properties, and conceptual properties. We test for group differences using paired t-tests or non-parametric Wilcoxon signed rank tests, when a Shapiro-Wilks test indicates that the variable is not normally distributed (summarized in Table \@ref(tab:analysis-summary-table)). Because this analysis involves multiple tests against the null hypothesis (*that there is no difference in the language input to blind vs. sighted kids*), we use the Benjamini-Hochberg correction [@benjamini1995] to control false discovery rate (Q = .05) for each set of analyses (quantity, interaction, linguistic, conceptual). Because each dimension's analysis consists of two statistical tests, our Benjamini-Hochberg critical values were *p* \< `r papaja::printnum((1/2 * .05),digits=3)` for the smaller *p* value and *p* \< `r papaja::printnum((2/2 * .05),digits=2)` for the larger *p* value. The results of these analyses are summarized in Table \@ref(tab:analysis-summary-table).

### Language Input Quantity

```{r compare-quantity}

AWC_shapiro <- shapiro.test(LENA_counts$AWC_per_hour)
compare_AWC <- t.test(LENA_counts$AWC_per_hour ~ LENA_counts$group, paired=TRUE)
AWC_stats <- LENA_counts %>%
  group_by(group) %>%
  summarise(min = min(AWC_per_hour),
            max = max(AWC_per_hour),
            mean = mean(AWC_per_hour))

MWC_shapiro<-shapiro.test(manual_word_tokens$MWC_per_hour)
compare_MWC <- t.test(manual_word_tokens$MWC_per_hour ~ manual_word_tokens$group, paired=TRUE)
MWC_stats <- manual_word_tokens %>%
  group_by(group) %>%
  summarise(min = min(MWC_per_hour),
            max = max(MWC_per_hour),
            mean = mean(MWC_per_hour))
```

We first compare the quantity of language input to blind and sighted children using two measures of the number of words in their environment: LENA's automated Adult Word Count and our transcription-derived Manual Word Count. Despite wide variability in the number of words children hear (Range from Manual Word Count: `r round((MWC_stats %>% filter(group=="VI"))$min)`--`r round((MWC_stats %>% filter(group=="VI"))$max)` words~blind~, `r round((MWC_stats %>% filter(group=="TD"))$min)`--`r round((MWC_stats %>% filter(group=="TD"))$max)` words~sighted~ per hour), along both measures of input quantity, blind and sighted children do not differ in language input quantity (Adult Word Count: *t*(`r compare_AWC$parameter`) = `r compare_AWC$statistic`, *p* `r printp(compare_AWC$p.value, add_equals=TRUE)`; Manual Word Count: *t*(`r compare_MWC$parameter`) = `r compare_MWC$statistic`, *p* `r printp(compare_MWC$p.value, add_equals=TRUE)`); see Figure \@ref(fig:quantity-plots).

```{r quantity-plots, fig.cap="Comparing LENA-generated adult word counts (left) and transcription-based word counts in the input of blind and sighted children. Violin density represents the distribution of word counts for each group. Grey lines connect values from matched participants. Black dot and whiskers show standard error around the mean. Neither measure differed between groups.", fig.width=6,fig.height=3, position="HOLD!"}


AWC_plot <- make_blind_sighted_violins(
  data = LENA_counts,
  x_var = "group",
  y_var = "AWC_per_hour",
  group_var = "group",
  y_label = "Adult Word Count (per hour)"
)

manual_word_count_plot <- make_blind_sighted_violins(
  data = manual_word_tokens,
  x_var = "group",
  y_var = "MWC_per_hour",
  group_var = "group",
  y_label = "Manual Word Count (per hour)"
)

quantity_title <- cowplot_title("Quantity Measures")

quantity_plots<- cowplot::plot_grid(AWC_plot, manual_word_count_plot,rel_widths = c(1,1),nrow = 1, labels = "AUTO")

cowplot::plot_grid(quantity_title, quantity_plots, rel_heights = c(0.1, 1), ncol=1)
```

### Interaction

```{r compare-interaction}
CTC_shapiro <- shapiro.test(LENA_counts$CTC_per_hour)
compare_CTC <- wilcox.test(LENA_counts$CTC_per_hour ~ LENA_counts$group, paired=TRUE)
CTC_stats <- LENA_counts %>%
  group_by(group) %>%
  summarise(min = min(CTC_per_hour),
            max = max(CTC_per_hour),
            mean = mean(CTC_per_hour))

CDS_shapiro <- shapiro.test(xds_props_wide$prop_CDS)
compare_CDS <- t.test(xds_props_wide$prop_CDS ~ xds_props_wide$group, paired=TRUE)
CDS_stats <- xds_props_wide %>%
  group_by(group) %>%
  summarise(min = min(prop_CDS),
            max = max(prop_CDS),
            mean = mean(prop_CDS))
```

Our corpus also revealed no significant difference in amount of interaction with the child, measured as the proportion of child-directed speech (*t*(`r compare_CDS$parameter`) = `r compare_CDS$statistic`, *p* `r printp(compare_CDS$p.value, add_equals=TRUE)`) or in conversational turn counts to blind children versus to sighted children (*W* = `r compare_CTC$statistic`, *p* `r printp(compare_CTC$p.value, add_equals=TRUE)`). Across both groups, child-directed speech constituted approximately `r round(mean(xds_props_wide$prop_CDS) * 100,0)`% of the input, and children were involved in roughly `r round(mean(LENA_counts$CTC_per_hour),0)` conversational turns per hour; see Figure \@ref(fig:interaction-plots).

```{r interaction-plots, fig.cap="Comparing LENA-generated conversational turn counts (left) and proportion of utterances in child-directed speech (center).  Violin density represents the distribution of values for each group. Grey lines connect values from matched participants. Black dot and whiskers show standard error around the mean. The full breakdown by addressee is shown in the rightmost panel. Neither conversational turn count nor proportion of child-directed speech differed between groups.", fig.width=7, fig.height=3}

CTC_plot <- make_blind_sighted_violins(
  data = LENA_counts,
  x_var = "group",
  y_var = "CTC_per_hour",
  group_var = "group",
  y_label = "Conversational Turn Count",
  title = NULL
)

CDS_prop_plot <- make_blind_sighted_violins(
  data = xds_props %>% filter(addressee == "CDS"),
  x_var = "group",
  y_var = "prop",
  group_var = "group",
  y_label = "Proportion of\nChild-Directed Speech",
  title = NULL
)

addressee_props_plot <- xds_props %>% filter(addressee!="TDS")%>%
  group_by(group, addressee) %>%
  summarise(prop = mean(prop,na.rm=TRUE)) %>%
  ggplot(aes(x=group,y=prop,fill=factor(addressee,levels=c("UDS","TDS","PDS","ODS","BDS","ADS","CDS")))) +
  geom_bar(stat="identity") +
  scale_fill_manual(name = "Addressee", breaks = c("CDS","ADS","BDS","ODS","PDS","UDS"),
                    labels = c("Child","Adult","Child & Adult", "Other", "Pet", "unknown"),
                    values = c("#FF70AE","#FFB870","#FFF170", "#BCE784","#ade7f5","#b16fff")
                    ) +
    geom_text(aes(label=case_when(prop>.05~round(prop,2))),
            position=position_stack(vjust=0.5)) +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted"))+
  theme_classic() +
  theme(text=element_text(size=10),
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("Proportion of Utterances\nBy Addressee")+
  xlab(NULL)


interaction_title <- cowplot_title("Interaction Measures")

interaction_plots<- cowplot::plot_grid(CTC_plot, CDS_prop_plot, addressee_props_plot, rel_widths = c(1,1.05, 1.5),nrow=1, labels = "AUTO")

cowplot::plot_grid(interaction_title, interaction_plots, rel_heights = c(0.1, 1.8), ncol=1)
```

### Linguistic Features

```{r linguistic-plots, fig.cap="Comparing linguistic features: Mean length of utterance (left) and type-token ratio (right). Violin density represents the distribution of values for each group. Grey lines connect values from matched participants. Black dot and whiskers show standard error around the mean. Utterances in blind children's input were significantly longer, and type-token ratio was significantly higher. Note that the y-axis on the type-token ratio plot has been truncated.", fig.height=3, fig.width=4.5}

TTR_plot <- make_blind_sighted_violins(
  data = TTR_calculations,
  x_var = "group",
  y_var = "mean_ttr",
  group_var = "group",
  y_label = "Type-Token Ratio:\nUnique Words / Total Words",
  y_axis_start=0.4
)


TTR_shapiro <- shapiro.test(TTR_calculations$mean_ttr)
TTR_test <- t.test(TTR_calculations$mean_ttr~TTR_calculations$group, paired=TRUE)
TTR_stats <- TTR_calculations %>%
  group_by(group) %>%
  summarise(mean=mean(mean_ttr),
            SE = std.error(mean_ttr))

raw_TTR_test <- t.test(raw_TTR_calculations$mean_ttr~raw_TTR_calculations$group, paired=TRUE)
raw_TTR_stats <- raw_TTR_calculations %>%
  group_by(group) %>%
  summarise(mean=mean(mean_ttr),
            SE = std.error(mean_ttr))

MLU_plot <- make_blind_sighted_violins(
  data = MLUs %>% filter(MLU > 0),
  x_var = "group",
  y_var = "MLU",
  group_var = "group",
  y_label = "Mean Length of Utterance\n(morphemes)",
  title = NULL,
)


MLU_stats <- MLUs %>%
  group_by(group) %>%
  summarise(mean=mean(MLU),
            SE = std.error(MLU))
MLU_shapiro <- shapiro.test(MLUs$MLU)
MLU_compare <- t.test(MLUs$MLU ~ MLUs$group, paired=TRUE)



linguistic_title <- cowplot_title("Linguistic Measures")

linguistic_plots<- cowplot::plot_grid(MLU_plot, TTR_plot, labels = "AUTO")

cowplot::plot_grid(linguistic_title, linguistic_plots, rel_heights = c(0.1, 1), ncol=1)

```

Similarly, neither linguistic variable differed across groups: blind and sighted children's input had comparable type-token ratios (*t*(`r TTR_test$parameter`) = `r TTR_test$statistic`, *p* `r papaja::printp(TTR_test$p.value, add_equals=TRUE)`) and utterance lengths (*t*(`r MLU_compare$parameter`) = `r MLU_compare$statistic`, *p* = `r printp(MLU_compare$p.value)`). Children in our samples hear on average `r round(mean(TTR_stats$mean),2) * 100` unique words per hundred words and `r round(mean(MLU_stats$mean),1)` morphemes per utterance; see Figure \@ref(fig:linguistic-plots).

### Conceptual Features

```{r sensory-modality}

visual_plot <- make_blind_sighted_violins(
  data = sensory_props_wide,
  x_var = "group",
  y_var = "prop_Visual",
  group_var = "group",
  y_label = "Proportion of Highly Visual Words"
)


sensory_props_plot <- sensory_props %>%
  group_by(group,Modality) %>%
  summarise(prop = mean(prop,na.rm=TRUE)) %>%
  ggplot(aes(fill = factor(Modality,levels=c("Amodal","Olfactory","Gustatory","Haptic","Interoceptive","Auditory","Visual","Multimodal")),
             y= prop,
             x = group)) +
  geom_bar(position = "stack", stat = "identity")  +
  geom_text(aes(label=case_when(prop>.04~round(prop,2))),
            position=position_stack(vjust=0.5)) +
  xlab("Group") +
  ylab ("Proportion of Words\nby Sensory Modality") +
  theme_classic()+
  theme(text=element_text(size=10),
    axis.text.x = element_text(angle = 45, hjust = 1)
        ) +
  scale_fill_manual(name="Modality",
                    breaks=c("Multimodal","Visual","Auditory","Interoceptive","Haptic","Gustatory","Olfactory", "Amodal"),
                    values=c("#FF70AE","#EF233C","#FFB870","#FFF170", "#BCE784","#ade7f5","#b16fff","#A5ACA5"))+
  scale_x_discrete(labels=c("VI" = "Blind", "TD" = "Sighted")) +
  xlab(NULL)

visual_shapiro <- shapiro.test(sensory_props_wide$prop_Visual)
visual_test<- wilcox.test(sensory_props_wide$prop_Visual ~ sensory_props_wide$group, paired=TRUE)
```

```{r verb-tense}

hereandnow_plot <- make_blind_sighted_violins(
  data = temporality_props_wide,
  x_var = "group",
  y_var = "prop_displaced",
  group_var = "group",
  y_label = "Proportion of Displaced Verbs",
)


temporality_props_plot <- temporality_props %>%
  mutate(group=as.factor(str_sub(VIHI_ID,1,2))) %>%
  group_by(group, verb_temporality) %>%
  summarise(prop = mean(prop, na.rm = TRUE)) %>%
  ggplot(aes(fill = factor(verb_temporality, levels=c("ambiguous","displaced","present")),
                                 y = prop,
                                 x = group)) +
  geom_bar(position = "stack", stat = "identity")  +
  ylab ("Proportion of Utterances\nby Verb Temporality") +
   geom_text(aes(label=round(prop,2)),
            position=position_stack(vjust=0.5)) +
  theme_classic() +
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(name="Verb Tense",
                    labels=c("Ambiguous", "Displaced", "Present"),
                    values=c("#A5ACA5","#FFF170","#BCE784"))+
  scale_x_discrete(labels =                                                   c("VI" = "Blind", "TD" = "Sighted")) +
  xlab(NULL)

temporality_shapiro<- shapiro.test(temporality_props_wide$prop_displaced)
temporality_test <- t.test(temporality_props_wide$prop_displaced ~ temporality_props_wide$group, paired=TRUE)
```

```{r conceptual-plots, fig.cap="Left col: Comparing proportion of temporally displaced verbs (top) and proportion of highly visual words (bottom). Violin density represents the distribution of values for each group. Grey lines connect values from matched participants. Black dot and whiskers show standard error around the mean. Right col: Full distribution of verb types (top)  and sensory modality (bottom) by group, collapsing across participants. Blind children's input contained significantly more temporally displaced verbs. Notably, the groups did not differ in the proportion of highly visual words.", fig.height=8, fig.width=7}

conceptual_title <- cowplot_title("Conceptual Measures")

conceptual_plots<- cowplot::plot_grid(hereandnow_plot,temporality_props_plot,visual_plot,sensory_props_plot,ncol = 2, rel_widths = c(1.5,2), labels = "AUTO")

cowplot::plot_grid(conceptual_title, conceptual_plots, rel_heights = c(0.1, 2), ncol=1)
```

Lastly, we compared two measures of the conceptual features of language input: the proportion of temporally displaced verbs and the proportion of highly visual words; see Figure \@ref(fig:conceptual-plots). We found that blind children hear a higher proportion of displaced verbs than sighted children (*t*(`r temporality_test$parameter`) = `r temporality_test$statistic`, *p* `r papaja::printp(temporality_test$p.value, add_equals=TRUE)`), which on average equates to `r round(abs(temporality_test$estimate) * (mean(temporality_props_wide$verb_utt_count)) * (60/40))` more utterances about past, future, or hypothetical events per hour. We found no significant difference across groups in the proportion of highly visual words (*W* = `r visual_test$statistic`, *p* `r printp(visual_test$p.value, add_equals=TRUE)`), which constitute roughly `r round((mean(sensory_props_wide$prop_Visual))*100)`% of the input for both groups.

```{r analysis-summary-table, results='asis'}

vars <-
  c(
    "Adult Word Count",
    "Manual Word Count",
    "Conversational Turn Count",
    "Prop. Child-Directed Speech",
    "Type-Token Ratio",
    "Mean Length of Utterance",
    "Prop. Displaced",
    "Prop. Visual"
  )
  `Portion of Recording` <- c("Whole day", "Random", "Whole day", "Random", "Random + High Volume",
                             "Random + High Volume", "Random + High Volume", "Random + High Volume")
  Description <- c("Estimated number of words in recording categorized as nearby adult speech by LENA algorithm",
                  "Number of word tokens from speakers other than target child",
                  "Count of temporally close switches between adult and target-child vocalizations, divided by recording length",
                  "Number of utterances tagged with child addressee out of total number of utterances, from speakers other than target child",
                  "Average of the type-token ratios (number of unique words divided by number of total words) for each of the 100-word bins in their sample",
                  "Average number of morphemes per utterance",
                  "Proportion of verbs that refer to past, future, or hypothetical events",
                  "Proportion of words in the input with high visual association ratings and low ratings for other perceptual modalities")

tests <-
  c(
    "t-test",
    "t-test",
    "Wilcoxon test",
    "t-test",
    "t-test",
    "t-test",
    "t-test",
    "Wilcoxon test"
  )
MeanBlind <-
  c(
    get_group_means(LENA_counts, "AWC_per_hour", group == "VI", unit = "words/hour",adjust=2, digits=0),
    get_group_means(manual_word_tokens, "MWC_per_hour", group == "VI", unit = "words/hour", adjust=2, digits=0),
    get_group_means(LENA_counts, "CTC_per_hour", group == "VI", unit = "turns/hour",adjust=2, digits=0),
    get_group_means(xds_props_wide, "prop_CDS", group == "VI"),
    get_group_means(TTR_calculations, "mean_ttr", group == "VI", unit = "unique words/ 100 words"),
    get_group_means(MLUs, "MLU", group == "VI", unit = "morphemes"),
    get_group_means(temporality_props_wide, "prop_displaced", group == "VI"),
    get_group_means(sensory_props_wide, "prop_Visual", group == "VI")
  )
MeanSighted <-
  c(
    get_group_means(LENA_counts, "AWC_per_hour", group == "TD", unit = "words/hour",adjust=2, digits=0),
    get_group_means(manual_word_tokens, "MWC_per_hour", group == "TD", unit = "words/hour", adjust=2, digits=0),
    get_group_means(LENA_counts, "CTC_per_hour", group == "TD", unit = "turns/hour",adjust=2, digits=0),
    get_group_means(xds_props_wide, "prop_CDS", group == "TD"),
    get_group_means(TTR_calculations, "mean_ttr", group == "TD", unit = "unique words/ 100 words"),
    get_group_means(MLUs, "MLU", group == "TD", unit = "morphemes"),
    get_group_means(temporality_props_wide, "prop_displaced", group == "TD"),
    get_group_means(sensory_props_wide, "prop_Visual", group == "TD")
  )
raw_ps <-
  c(
    compare_AWC$p.value,
    compare_MWC$p.value,
    compare_CDS$p.value,
    compare_CTC$p.value,
    TTR_test$p.value,
    MLU_compare$p.value,
    temporality_test$p.value,
    visual_test$p.value
  )
ps_df <- bind_cols(vars, Description, `Portion of Recording`, tests, MeanBlind, MeanSighted, raw_ps) %>%
  dplyr::rename(
    "Variable" = `...1`,
    "Portion of Recording" = `...3`,
    "Description" = `...2`,
        "Test" = `...4`,
        "Blind Mean, Median, Range" = `...5`,
    "Sighted Mean, Median, Range" = `...6`,
    "p value" = `...7`
  ) %>%
  mutate(`p value` = case_when(`p value` < .05 ~ paste0(printp(`p value`), "*"),
                               TRUE ~ printp(`p value`)))
kable(ps_df, caption = "Summary of language input variables: how the measure is calculated; what portion of the recording the measure was calculated over; whether a parametric or non-parametric test was used; the mean, median, and range for blind and sighted children, and the raw (uncorrected) p-value of the test comparing groups. Only prop. displaced reached significance at our corrected p<.025 threshold for significance.", align = c("l", "l", "l", "l", "l", "l","c")) %>%
  kable_styling(font_size = 9) %>%
  column_spec(1, width = ".82in") %>%
    column_spec(2, width = "2in") %>%
  column_spec(3, width = ".76in") %>%
    column_spec(4, width = ".6in") %>%
    column_spec(5, width = ".7in") %>%
  column_spec(6, width = ".8in") %>%
  column_spec(7, width = ".43in") 

```

# Discussion

In this study, we analyzed the everyday language input to 15 young congenitally-blind children alongside a carefully peer-matched sighted sample using LENA audio recorders. While still relatively modest in absolute terms, this is a larger and more naturalistic sample than has previously been leveraged by prior work with this low-incidence population. We found that along the quantity, interaction, and linguistic dimensions, caregivers talked similarly to blind and sighted children, with small but potentially notable differences in conceptual content of the input. We discuss each of these results further below.

## Quantity

Across two measures of language input quantity, one estimated from the full sixteen hour recording (Adult Word Count) and one precisely measured from a 30-minute samples from the day (Manual Word Count), blind and sighted children were exposed to similar amounts of speech in the home. Quantity was highly variable *within* groups, but we found no evidence for *between* group differences in input quantity. This runs counter to two folk accounts of language input to blind children: 1) that sighted parents of blind children might talk *less* because they don't share visual common ground with their children; 2) that parents of blind children might talk *more* to compensate for their children's lack of visual input. Instead, we find a similar quantity of speech across groups.

## Interaction

We quantified interaction in two ways: through the LENA-estimated conversational turn count and through the proportion of child-directed speech in our manual annotations. Again, we found no differences across groups in the amount of parent-child interaction. This finding contrasts with previous research; other studies report *less* interaction in dyads where the child is blind [@rowland1984; @perez-pereira2001; @moore1994; @kekelis1984; @preisler1991; @andersen1993; @grumi2021]. Using a non-visual sampling method (i.e., our audio recordings) might provide a different, more naturalistic perspective on parent-child interactions, particularly in this population. For one thing, many prior studies [e.g., @kekelis1984; @preisler1991; @moore1994; @perez-pereira2001] involve videorecordings in the child's home, with the researcher present. Like other young children, blind children distinguish between familiar individuals and strangers, and react with trepidation to the presence of a stranger; for blind children, this reaction may involve "quieting", wherein children cease speaking or vocalizing when they hear a new voice in the home [@mcrae2002; @fraiberg1975]. By having a researcher present during the recordings[^6], prior research may have artificially suppressed blind children's initiation of interactions. Even naturalistic, observer-free videorecordings appear to inflate aspects of parental input, relative to daylong audio recordings [@bergelson2019]. Together, these factors could explain why past parent-child interaction research finds that blind children initiate fewer interactions [@andersen1993; @kekelis1984; @dote-kwan1995; @troster1992; @moore1994], that parents do most of the talking [@kekelis1984; @andersen1993], and that there is overall less interaction [@rowland1984; @nagayoshi2017; @rogers1984; @troster1992].

[^6]: Fraiberg (1975) writes "these fear and avoidance behaviors appear even though the observer, a twice-monthly visitor, is not, strictly speaking, a stranger." (pg. 323).

Additionally, a common focus in earlier interaction literature is to measure visual cues of interaction, such as shared gaze or attentiveness to facial expressions [@preisler1991; @baird1997; @rogers1984]. We can't help but wonder: are visual markers of social interaction the right yardstick to measure blind children against? In line with @macleod2023, perhaps the field should move away from sighted indicators of interaction "quality", and instead situate blind children's interactions within their own developmental niche, one that may be better captured with auditory- or tactile-focused measures.

## Linguistic Features

Along the linguistic dimension, we measured type-token ratio and mean length of utterance. Parents of children with disabilities [including parents of blind children, e.g., @chernyak; @familyconnect] are often advised to use shorter, simpler sentences with their children; correspondingly, previous work finds that parents of children with disabilities tend to find that parents *do* use shorter, simpler utterances [e.g., Down syndrome, @lorang2020; hearing loss, @dirks2020]. We therefore expected to observe shorter utterances and less lexical diversity in speech to blind vs. sighted children. Instead, we found that blind children heard indistinguishable input by these metrics, with, if anything, a (marginally significant) trend towards *longer* sentences in their input (relative to sighted matches, roughly half a word longer on average). Returning to the potential impact of input properties on children, evidence suggests that (contrary to the advice often given to parents), longer, more complex utterances are associated with better child language outcomes in both typically-developing children [@hoff2002] and children with cognitive differences [@sandbank2016]. And similarly, higher lexical diversity is associated with larger vocabulary [@rowe2012; @anderson2021; @huttenlocher2010; @hsu2017; @weizman2001]. Regardless, the present analysis did not reveal robust statistical evidence that, at least on the group level, caregivers systematically provide utterances with different length or lexical diversity as a function of whether their child could see.

## Conceptual Features

Although there are many potential ways to measure the conceptual features of language, we chose to capture *here-and-now*-ness by measuring the proportion of temporally displaced verbs and the proportion of highly visual words. We found that blind children heard roughly 5% more temporally displaced verbs than sighted peers. Moreover, though blind and sighted participants were exposed to a similar proportion of highly visual words, the referents of these words are by definition, inaccessible to the blind participants. Taken together, our conceptual results suggest that blind children's input is *less* focused on the *here-and-now*.

The extent to which blind children's language input is centered on the *here-and-now* has been contested in the literature [@urwin1984; @moore1994; @andersen1993; @campbell2003; @kekelis1984]. This aspect of language input is of particular interest because early reports suggest that blind children's own use of decontextualized language develops later than sighted children's [@urwin1984; @bigelow1990]. Could such a difference be attributable to an absence of decontextualized language in the input? Our results suggest this is unlikely: we find that blind children's input contains *more* decontextualized language rather than less. Speculatively, this may be because blind children have less access to immediate visual cues, leading to caregivers more frequently referring to past or future events to engage with their child. To illustrate, while riding on a train, instead of describing the scenery passing outside the window, parents may choose to talk about what happened earlier in the day or their plans upon arriving home. Without further information about the social and perceptual context, it is difficult to determine the communicative function of the differences we find in conceptual features we find or how they might explain differences in children's decontextualized language use. As more dense annotation becomes available, we look forward to further work exploring the social and environmental contexts of conceptual information as it unfolds across discourse.

It is worth underscoring again how much variability there is *within* groups and how much consistency there is *between* groups. One could imagine a world in which the language environments of blind and sighted children are radically different from each other. Our data do not support that hypothesis. Rather, we find similarity in quantity, interaction, and linguistic properties, alongside modest differences in conceptual properties. That is, in line with recent work highlighting immense *within*-group variability across many different socio-cultural and linguistic contexts [@bergelson2023], our blind and sighted groups here have large within-group variability but very few between-group differences. Despite strikingly different visual experiences, young blind and sighted learners have at best modest differences in their speech environments.

## Connecting to Language Outcomes

Our results uncover no systematic group differences in the quantity of speech, amount of language interaction, or linguistic complexity parents provide to blind vs. sighted children, at least as measured here. When we do see differences, language input to blind children looks more conceptually complex or perceptually unavailable. In other populations, complexity of this sort is linked with *more* sophisticated child language outcomes [@rowe2012; @demir2015; @uccelli2019], so it is not the case that blind children's language input is "impoverished" in this sense.

In our modestly-sized, predominantly pre-lexical sample, linking language input to children's language outcomes directly is not yet feasible, but prior literature allows us to speculate on two possibilities. First, if input effects pattern similarly for blind and sighted children, we would expect blind and sighted children alike to benefit from more input [@gilkerson2018; @huttenlocher1991; @rowe2008; @anderson2021], more interactive input [@goldstein2008; @donnellan2020; @hirsh-pasek2015; @romeo2018; @weisleder2013; @rowe2008; @shneidman2013], more linguistically complex input [@rowe2012; @anderson2021; @huttenlocher2010; @hsu2017; @weizman2001; @hoff2003; @naigles1998; @devilliers1985; @hadley2017; @huttenlocher2002], and more conceptually complex input [@rowe2012; @demir2015; @uccelli2019].

At the same time, however, recent results show that blind children have a roughly half-year delay in their productive vocabulary, relative to sighted peers [@campbell2024]. If properties of the language input play a role in this delay, this raises the second possibility: that language input affects acquisition *differently* for blind children than it does for sighted children. Under this possibility, blind children would benefit from *less* complex language input, and the equivalencies in quantity, linguistic complexity and interactivity alongside the increased conceptual complexity we find here would, in theory, contribute to early vocabulary delays.

To show our cards, we are inclined towards option one: that blind children benefit from language input in the same ways as sighted peers [@landau1985], and that this additionally extends to the benefits of receiving more conceptually complex language input. Language regularly supports learning in the absence of direct sensory perception (e.g., reading a book about mythical creatures). Given the language skills of blind adults [@roder2000; @roder2003; @loiotile2020], it is undeniable that language is a rich source of meaning for blind individuals as well [@lewis2019; @vanparidon2021; @campbell2022]. Testing each of these predictions--as well as whether links between language input and language outcomes change across developmental time--awaits further research.

In either case, if properties of language input do influence blind children's language outcomes, attempting to train parents to talk differently may be unfruitful. While some interventions where parents are trained to talk differently to their children show promise [@roberts2019; @huber2023], such interventions often fail to change parental speech patterns on more extended timescales [e.g., @mcgillion2017; @suskind2016].

# Conclusion

In summary, our study compared language input in homes of 15 blind and 15 sighted infants. We found that both groups received language input with similar quantities of speech, interactivity, and linguistic complexity. Additionally, blind children were exposed to input that had somewhat more conceptual complexity, in terms of discussion beyond the here-and-now and words for less perceptually-avaiable (visual) referents. This suggests that young blind children are being exposed to a rich linguistic environment that differs only modestly from the language input of sighted children. Our study does not imply that parents should change their communication styles, but rather highlights the language experiences of blind children. Future research linking input links to language development and cognitive abilities of blind and sighted children alike would be a fruitful and welcome next step.

```{r trackdown}
# trackdown::update_file(file="input_quality_manuscript.Rmd",gpath="trackdown/input_quality",hide_code=TRUE, path_output = "input_quality_manuscript.pdf")
```

# Ethics

This study received approval from the Duke University Institutional Review Board. All families consented to take part in this research.

# Data, Code and Materials Availability

LENA data, transcripts, and code for all analyses presented in this article are available on [OSF](https://osf.io/dcnq6/).

# Authorship

**Erin Campbell:** Conceptualization, Validation, Investigation, Analysis, Data Curation, Writing - Original Draft, Writing - Review & Editing, Visualization, Project Administration; **Lillianna Righter:** Validation, Analysis, Data Curation, Writing - Original Draft, Writing - Review & Editing, Project Administration; **Eugenia Lukin:** Validation, Writing - Review & Editing; **Elika Bergelson:** Conceptualization, Resources, Writing - Review & Editing, Visualization, Supervision, Funding Acquisition

# Acknowledgements

We wish to thank the following individuals who generously contributed recordings for the sighted matches: Anne Warlaumont, Derek Houston, Nairn Ramrez-Esparza, Mark VanDam, Caroline Rowland. We thank Zhenya Kalenkovich and Alex Emmert for careful code review.

# References
