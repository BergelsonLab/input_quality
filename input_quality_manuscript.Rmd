---
title             : "Comparing Language Input in Homes of Blind and Sighted Children: Insights from Daylong Recordings"

shorttitle        : "Language Input to Blind and Sighted"

author:
  - name          : "Erin Campbell"
    affiliation   : "1"
    corresponding : yes    
    email         : "erin.e.campbell@duke.edu"
    address       : "417 Chapel Drive, Box 90086, Durham, NC 27708"
  - name          : "Lillianna Righter"
    affiliation   : "1"
  - name          : "Eugenia Lukin"
    affiliation   : "1"
  - name          : "Elika Bergelson"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology & Neuroscience, Duke University, Durham, NC"

note: |
  **Conflicts of Interest**: The authors have no conflicts of interest to report.
  **Funding**: This work was supported by the National Science Foundation CAREER grant (BCS-1844710) to EB and Graduate Research Fellowship (2019274952) to EC.

keywords          : "keywords"
wordcount         : "X"

bibliography      : ["VI_input_citations.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r libraries, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
library(ggplot2)
library(plotrix)
library(see)
library(papaja)
library(stringr)
library(morphemepiece)
library(tidytext)
library(reshape2)
library(profileR)
library(lubridate)
library(corrplot)
library(psych)
library(glue)
library(knitr)
library(kableExtra)
library(Table1)

#lets create a function to convert things into percents (I stole this obvi)
percent <- function(x, digits = 1, format = "f", ...) {      # Create user-defined function
  paste0(formatC(x * 100, format = format, digits = digits, ...), "%")
}
seconds_to_length <- function(seconds, format = "f", ...) {      
  paste0(hour(seconds_to_period(seconds)), " hours ", minute(seconds_to_period(seconds)), " minutes")
}

compare_corrs  <- function(var1, var2) {    
  subset_VI <- lotta_data %>% filter(group=="VI")
subset_TD <- lotta_data %>% filter(group=="TD")  
  correlation_blind <- cor.test(subset_VI[[var1]],subset_VI[[var2]], method="kendall")
correlation_sighted <- cor.test(subset_TD[[var1]],subset_TD[[var2]], method="kendall")

glue("*r*~blind~ = {printnum(correlation_blind$estimate)}, *p*~blind~ {papaja::printp(correlation_blind$p.value,add_equals=TRUE)}; *r*~sighted~ = {printnum(correlation_sighted$estimate)}, *p*~sighted~ {papaja::printp(correlation_sighted$p.value,add_equals=TRUE)}")
}

report_kcorrelation  <- function(var1, var2, subset=FALSE,subset_label="") {
  if (subset == FALSE) {
  results <- cor.test(lotta_data[[var1]], lotta_data[[var2]], method="k")
  glue("*r* = {papaja::printnum(results$estimate)}, *p* {papaja::printp(results$p.value,add_equals=TRUE)}")}
  else {
    subsetted <- lotta_data %>% filter(group==subset)
    results <- cor.test(subsetted[[var1]], subsetted[[var2]], method="k")
    glue("*r*~{subset_label}~ = {papaja::printnum(results$estimate)}, *p*~{subset_label}~ {papaja::printp(results$p.value,add_equals=TRUE)}")
  }

}

preprocess <- TRUE
new_graphs <- FALSE

if (preprocess == TRUE) {source("input_quality_preprocessing.R")} else {
# quantity
LENA_counts <- read.csv("data/LENA/Automated/LENA_counts.csv")
manual_word_TTR <- read.csv("data/LENA/Transcripts/Derived/manual_word_TTR.csv")
manual_word_tokens <- read.csv("data/LENA/Transcripts/Derived/manual_word_tokens.csv")
# interactive
xds_props <- read.csv("data/LENA/Transcripts/Derived/xds_props.csv")
xds_props_wide <- read.csv("data/LENA/Transcripts/Derived/xds_props_wide.csv")
# linguistic
MLUs <- read.csv("data/LENA/Transcripts/Derived/MLUs.csv")
TTR_calculations <- read.csv("data/LENA/Transcripts/Derived/TTR_calculations.csv")
# conceptual
sensory_props <- read.csv("data/LENA/Transcripts/Derived/sensory_props.csv")
sensory_props_wide <- read.csv("data/LENA/Transcripts/Derived/sensory_props_wide.csv")
temporality_props <- read.csv("data/LENA/Transcripts/Derived/temporality_props.csv")
temporality_props_wide <- read.csv("data/LENA/Transcripts/Derived/temporality_props_wide.csv")
subj_CBOI_means <- read.csv("data/LENA/Transcripts/Derived/subj_CBOI_means.csv")
content_words_only <- read.csv("data/LENA/Transcripts/Derived/content_words_only.csv")
annotated_utterances <- read.csv("data/LENA/Transcripts/Derived/annotated_utterances.csv")
verbs_only <- read.csv("data/LENA/Transcripts/Derived/verbs_only.csv")
}

n_uncat<-sum(verbs_only$temporality == "uncategorized")

#demo
VI_matches_demo <- read.csv("data/Demographics/VI_matches_demo.csv", na = c("", "NA", " "))
VIHI_transcripts_messy <- read.csv("data/LENA/Transcripts/Raw/VI_LENA_and_TD_matches_messy2023-05-22.csv")

lotta_data <- LENA_counts %>%
  left_join(MLUs, by=c("VIHI_ID", "group"))  %>%
  left_join(TTR_calculations, by=c("VIHI_ID", "group")) %>%
  left_join(xds_props_wide %>% dplyr::select(-total), by=c("VIHI_ID", "group")) %>%
  left_join(sensory_props_wide %>% dplyr::select(-total), by=c("VIHI_ID", "group"))%>%
  left_join(subj_CBOI_means, by=c("VIHI_ID", "group")) %>%
  left_join(temporality_props_wide, by=c("VIHI_ID", "group")) %>%
  mutate(ParticipantNumber = as.factor(str_sub(VIHI_ID, 1, 6))) %>%
  mutate(LENA_age_in_days = as.numeric(str_sub(VIHI_ID, 8, length(VIHI_ID))))%>%
  distinct(ParticipantNumber, .keep_all=TRUE)
```

```{r get-data}

participant_summary <- LENA_counts %>%
  mutate(Age_days = as.numeric(str_sub(VIHI_ID, 8, length(VIHI_ID))),
         Age_months = Age_days * 0.0328767) %>%
  group_by(group) %>%
  summarize(age = mean(Age_months,na.rm=TRUE),
            min_age = min(Age_months),
          max_age = max(Age_months),
          N=n())

```

# Abstract

**Purpose:** This study compared language input to young blind children and their sighted peers in naturalistic home settings.

**Methods:** Using LENA audio recordings, naturalistic speech in the home was captured, transcribed, and analyzed for various dimensions of language input, including quantitative, interactive, linguistic, and conceptual features.

**Results:** Our data showed broad similarity across groups in speech quantity and interaction. Fine-grained analysis revealed that blind children's language environments contained more lexical diversity, longer utterances, and more temporal displacement.

**Conclusions:** The findings challenge the notion that blind children's language input places them at a disadvantage and suggest instead that blind children receive rich and complex language input that can support their language development.

# Introduction

The early language skills of blind children are highly variable [@campbellsubmitted]. Some demonstrate age-appropriate vocabulary from the earliest stages of language learning [@landau1985; @bigelow1987; @campbellsubmitted], while others experience substantial language delays [@campbellsubmitted]. By adulthood, however, blind individuals are fluent language-users, demonstrating faster lexical processing skills than sighted adults [@roder2000; @roder2003; @loiotile2020]. The causes of early variability and the later ability to "catch up" remain poorly understood: what could make the language learning problem different or initially more difficult for the blind child? Here, we compare the language environments of blind children to that of their sighted peers. In doing so, we begin to untangle the role that perceptual input plays in shaping children's language environment, and better understand the interlocking factors that may contribute to variability in blind children's early language abilities.

## Why would input matter?

Among both typically-developing children and children with developmental differences, language input can predict variability in language outcomes [@gilkerson2018; @huttenlocher1991; @rowe2008; @anderson2021; @rowe2012; @anderson2021; @huttenlocher2010]. The many ways to operationalize language input tend to be grouped into **quantity of language input** and **input characteristics** [often referred to as **quality of language input**[^1], c.f. @macleod2023]. Quantity can be broadly construed as the number of words or utterances a child is exposed to. At a coarse level, children who are exposed to more speech [or sign, @watkins1998] tend to have better language outcomes [@gilkerson2018; @huttenlocher1991; @rowe2008; @anderson2021]. 

[^1]: In the present study, we move away from describing these linguistic characteristics as "quality" measures. In the field thus far, the directionality of the term "quality" has favored the types of language used by white and abled groups as immutable universal standards, thereby framing racialized and disabled peoples' language as deficient and "low quality" by nature. Describing a singular source of input variation as "high quality" ignores the sociocultural variation of talk styles, and the presence of many rich sources of information from which children can learn (MacLeod & Demers, 2023).

The specific characteristics of language input are perhaps even more influential [@rowe2012; @hirsh-pasek2015], although they are somewhat trickier to quantify into discrete measures. Rowe and Snow [@rowe2020] divide this space into three dimensions: interactive features (e.g., parent responsiveness, speech directed *to* child vs. overheard, conversational turn-taking), linguistic features (e.g., lexical diversity, grammatical complexity), and conceptual features (i.e., the extent to which input focuses on the *here-and-now*).

Prior literature reports that back-and-forth communicative exchanges (also known as conversational turns) between caregivers and children predict better language outcomes across infancy [@goldstein2008; @donnellan2020] and toddlerhood [@hirsh-pasek2015; @romeo2018]. Another way to quantify the extent to which caregivers and infants interact is by looking at how much speech is directed *to* the child (as opposed to, for example, an overheard conversation between adults). The amount of child-directed speech in children's input [at least in Western contexts, @casillas2020] is associated with children's vocabulary size and lexical processing [@weisleder2013; @rowe2008; @shneidman2013].

Under the linguistic umbrella, we can measure the *kinds* of words used (often measured as lexical diversity, type/token ratio), and the ways they are *combined* (syntactic complexity, often measured by mean length of utterance) . Both parameters have measurable associations with children's language growth. Sighted toddlers who are exposed to a greater diversity of words in their language input are reported to have larger vocabulary scores [@rowe2012; @anderson2021; @huttenlocher2010; @hsu2017; @weizman2001]. Likewise, the diversity and complexity of syntactic constructions in parental language input is associated with both children's vocabulary growth and structural diversity in their own productions [@hoff2003; @naigles1998; @devilliers1985; @huttenlocher2010; @hadley2017; @huttenlocher2002].

The conceptual dimension of language input aims to capture the extent to which the language signal maps onto present objects and ongoing events in children's environments [@rowe2020]. As children develop, their ability to represent abstract and abstract referents improves [@luchkina2020; @kramer1975; @bergelson2013]. Decontextualized language input-- that is, talking about past, future, or hypothetical events, or people and items that are not currently present in the environment-- may be one contributing factor [@rowe2013]; greater amounts of decontextualized language use in input to toddlers predict aspects of children's own language in kindergarten and beyond [@rowe2012; @demir2015; @uccelli2019].

From this review, it appears that many factors in the language input alone influence how sighted children learn about the world and language, but they also learn from sensory perception and conceptual and social knowledge. Many cues for word learning are visual: sighted children can utilize visual information like parental gaze, shared visual attention [@tomasello1986], pointing [@lucca2018], and the presence of salient objects in the visual field [@yu2012]. Because these visual cues are inaccessible to blind children, language input may take on a larger role in the discovery of word meaning [@campbell2022]. Syntactic structure in particular may provide cues to word meaning that may be lost, such as the relationship between two entities that aren't within reach [@gleitman1990]. However, such hypotheses implicitly assume that blind and sighted children receive similar language input.

## Why would the input differ between blind and sighted children?

Speakers regularly tailor their speech to communicate efficiently with the listener [@grice1975]. Parents are sensitive to their child's developmental level and tune language input accordingly [@snow1972; @vygotsky1978]. One example is child-directed speech, wherein parents speak to young children with exaggerated prosody, slower speech, and increased vowel clarity [@fernald1989; @bernsteinratner1984], which are in some cases helpful to the young language learner [@thiessen2005]. For instance, parents repeat words more often when interacting with infants than with older children or adults [@snow1972]. Communicative tailoring is also common in language input to children with disabilities, who tend to receive simplified, more directive language input, and less interactive input compared to typically-developing children [@dirks2020; @yoshinaga-itano2020].

In addition to tailoring communication to children's developmental level, speakers also adjust their conversation in accordance to their partner's sensory access [@gergle2004; @grigoroglou2016]. In a noisy environment, speakers adapt the acoustic-phonetic features of their speech to make it easier for their interlocutor to understand them [@hazan2011], demonstrating sensitivity to even temporary sensory conditions. When describing scenes, speakers aim to provide the information their listeners lack but avoid redundant visual description [@ostarek2019; @grice1975]. During in-lab tasks with sighted participants, participants verbally provide visually-absent cues when an object is occluded to their partner [@jara-ettinger2021; @hawkins2021; @rubio-fernandez2019]. These results suggest that adults and even infants [@senju2013; @ganea2018; @chiesa2015] can flexibly adapt communication to the visual and auditory abilities of their partner.

Taking these results into account, we might expect parents to verbally compensate for missing visual input, perhaps providing more description of the child's environment. But prior research doesn't yield a clear answer. Several studies suggest differences in the conceptual features: caregivers of blind children restrict conversation to things that the blind child is currently engaged with, rather than attempt to redirect their attention to other stimuli [@andersen1993; @campbell2003; @kekelis1984; though c.f., @moore1994]. Studies of naturalistic input to blind children report that parents use *fewer* declaratives and *more* imperatives than parents of sighted children, suggesting that blind children might be receiving less description than sighted children [@kekelis1984; @landau1985]. Other studies report that parents adapt their interactions to their children's visual abilities, albeit in specific contexts. @tadic2013 find that in a structured book reading task, parents of blind children provide more descriptive utterances than parents of sighted children. Further, parents of blind children provide more tactile cues to initiate interactions or establish joint attention [@urwin1983; @urwin1984; @preisler1991], which may serve the same social role as shared gaze in sighted children. These mixed results suggest that parents of blind children might alter language input in some domains but not others. The apparent conflict in results may be exacerbated by the difficulty of recruiting specialized populations to participate in research: the small (in most cases, single-digit) sample sizes of prior work limits our ability to generalize about any principled differences in the input to blind infants.

## The Present Study

If properties of language input influence the likelihood of language delays among blind infants and toddlers [@campbellsubmitted], capturing this variation may reveal a more nuanced picture of how infants use the input to learn language. In the present study, we examine daylong recordings of the naturalistic language environments of blind and sighted children in order to characterize the input to each group. Using both automated measures and manual transcription of these recordings, we measure input quantity (adult word count) and analyze several characteristics that have been previously suggested to be information-rich learning cues, including interaction (conversational turn counts, proportion of child-directed speech), conceptual features (temporal displacement, sensory modality), and linguistic complexity (type/token ratio and mean length of utterance).

# Methods

## Participants

`r LENA_counts %>% filter(group=="VI") %>% distinct(VIHI_ID, .keep_all =TRUE) %>% nrow()` blind infants and their families participated in this study. Blind participants were recruited through ophthalmologist referral, preschools, early intervention programs, social media, and word of mouth. To be eligible for this study, participants had to be 6--30 months old, have no additional disabilities (developmental delays; intellectual disabilities, or hearing loss), and be exposed to $\geq$ 75% English at home. To control for the wide age range of the study, each blind participant was matched to a sighted participant, based on age ($\pm$ 6 weeks), gender, maternal education ($\pm$ one education level), and number of siblings ($\pm$ 1 sibling). Caregivers were asked to complete a demographics survey and the MacArthur-Bates Communicative Development Inventory [CDI, @fenson1994] within one week of the home language recording. See Table \@ref(tab:participant-characteristics) for sample characteristics.

```{r participant-characteristics, results="asis"}
perc_random_silent <-
  ((
    VIHI_transcripts_messy %>% filter(
      !is.na(sampling_type) &
        sampling_type == "random" &
        is_silent == "Y"
    ) %>% nrow()
  ) / (
    VIHI_transcripts_messy %>% filter(!is.na(sampling_type) &
                                        sampling_type == "random") %>% nrow()
  )
  ) * 100

report_continuous <- function(vector, string,dig=1){
  range <- glue("{round(min(vector,na.rm=TRUE))}--{round(max(vector,na.rm=TRUE))}")
  mean <- glue("{round(mean(vector,na.rm=TRUE),dig)}")
  sd = glue("{round(sd(vector,na.rm=TRUE),dig)}")
  template <- "{range}{string},\n{mean} ({sd}){string}"
  glue(template, range = range, mean = mean, sd = sd)
}

level_template <- "{level}: {pct}%"
report_level <- function(vector, level) {
  fac = as.factor(vector)
  n <- sum(fac == level, na.rm = TRUE)
  total <- length(fac)
  pct <- round(n / total * 100)
  glue(level_template, n = n, pct = pct, level = level)
}
report_factor <- function(vector) {
  levels(as.factor(vector)) %>%
    map_chr(function(level) report_level(vector = vector, level = level)) %>%
    str_c(collapse = ',\n')
}

VI_matches_demo %>%
  mutate(match_group = as.factor(case_when(
    match_group == "VI_TD" ~ "Sighted (N=15)",
    match_group == "VI" ~ "Blind (N=15)")
  ),
  Race = if_else(Race == "Black or African American,White", "Mixed", Race),
MaternalEd = factor(MaternalEd, levels = c("Some college", "Associate's degree",
 "Bachelor's degree", "Master's degree",
 "Doctoral degree"
 )),
  Gender = recode(Gender, "F" = "Female", "M" = "Male"))  %>%
  dplyr::rename(Group = match_group) %>%
  mutate(
    across(c(Gender,Race), ~replace_na(.x, "Missing")),
           Gender = factor(Gender,levels=c("Female","Male")),
) %>%
  group_by(Group) %>%
summarize(
    `Age (months)` = report_continuous(Age_months, ""),
     Sex= report_factor(Gender),
     Race=report_factor(Race),
    `Number of Older Siblings` = report_continuous(OlderSiblings, ""),
    `Maternal Education Level`=report_factor(MaternalEd),
    Diagnosis = report_factor(Diagnosis)
  ) %>%
kable(caption = "Demographic characteristics of the blind and sighted samples") %>%
  kable_styling(font_size = 8) %>%
 column_spec(1, width = ".4in")%>%
 column_spec(2, width = ".5in")%>%
 column_spec(3, width = ".7in")%>%
 column_spec(4, width = ".75in")%>%
 column_spec(5, width = ".75in")%>%
  column_spec(6, width = "1in") %>%
column_spec(7, width = "1.5in")
```

## Recording Procedure

For the recording portion of the study, caregivers of participating infants received a LENA wearable audio recorder and vest [@ganek2016; @gilkerson2008]. They were instructed to place the recorder in the vest on the day of their scheduled recording and put the vest on their child from the time they woke up until the recorder automatically shut off after 16 hours (setting the vest nearby during baths, naps, and car rides). Actual recording length ranged from `r seconds_to_length(min(LENA_counts$total_time_dur))` to `r seconds_to_length(max(LENA_counts$total_time_dur))` (Mean: `r seconds_to_length(mean(LENA_counts$total_time_dur, na.rm=TRUE))`).

## Processing

The audio recordings were first processed by the LENA proprietary software [@xu2009], creating algorithmic measures such as conversational turn counts and adult word count. Each recording was then run through an in-house automated sampler that selected 15- non-overlapping 5-minute segments, randomly distributed across the duration of the recording. Each segment consists of 2 core minutes of annotated time, with 2 minutes of listenable context preceding the annotation clip and 1 minute of additional context following. Because these segments were sampled randomly, across participants roughly `r round(perc_random_silent)`% of the random 2-minute coding segments contained no speech at all. For questions of *how much does a phenomenon occur*, random sampling schemes can help avoid overestimating speech in the input, but for questions of input *content*, randomly selected samples may be too sparse [@pisani2021].

Therefore, we chose to annotate 5 additional segments specifically for their high density of speech. To select these segments of dense talk, we first conducted an automated analysis of the audio file using the voice type classifier for child-centered daylong recordings [@lavechin2021] which identified all human speech in the recording. The entire recording was divided into 2-minute chunks, each ranked highest to lowest by the total duration of speech contained within the chunk. We annotated the highest-ranked 5 segments of each recording. These high volubility segments allow us to more closely compare our findings to studies classifying the input during structured play sessions, which paint a denser and differently-proportioned makeup of the language input [@bergelson2019]. In sum, 30 minutes of randomly sampled input and 10 minutes of high-volubility input (40 minutes total) were annotated per child.

## Annotation

Recordings were annotated using the ELAN software [@brugman2009]. Trained annotators listened through each 2-minute segment plus its surrounding context and coded it using the ACLEW annotation scheme [@soderstrom2021]. For more information about this scheme, see the [ACLEW homepage](https://sites.google.com/view/aclewdid/home "ACLEW homepage"). Speech by people other than the target child was transcribed using an adapted version of the CHAT transcription style [@macwhinney2019; @soderstrom2021]. Because the majority of target children in the project are pre-lexical, utterances produced by the target child are not yet transcribed. Environmental speech was then classified by the addressee of each utterance: child, adult, both an adult and a child, pets or other animals, unclear addressee, or a recipient that doesn't fit into another category (e.g., voice control of Siri or Alexa, prayer to a metaphysical entity). Following the first pass, all files were reviewed by a highly-trained "superchecker" to ensure consistency 

## Extracting Measures of Language Input

To go from our dimensions of interest (quantity, interactiveness, linguistic, conceptual), to quantifiable properties, we used a combination of automated measures [generated by the proprietary LENA algorithm, @xu2009] and manual measures (generated from the transcriptions made by our trained annotators). Quantity and interactiveness analyses were conducted on the random samples only, to capture a more representative estimate. Linguistic and conceptual analyses were conducted on all available annotations in order to maximize the amount of speech over which we could calculate them. These measures are described below and summarized in Table \@ref(tab:ps).

### Quantity

#### Adult Word Count

To derive this count, LENA algorithm segments the recording into clips which are then classified by speaker's gender (male/female), age (child/adult), and distance (near/far), as well as several non-human speaker categories (e.g., silence, electronic noise). Only segments that are classified as nearby male or female adult speech are included in the Adult Word Count estimation [@xu2009]. Validation work suggests that this automated count correlates strongly with word counts derived from manual annotations [r = .71 -- .92, @lehet2021], and meta-analytic work finds that AWC is associated with children's language outcomes across developmental contexts [e.g., autism, hearing loss, @wang2020]. Because the recordings varied in length ( `r seconds_to_length(min(LENA_counts$total_time_dur))` to `r seconds_to_length(max(LENA_counts$total_time_dur))`), we normalized AWC by dividing by recording length[^2].

[^2]: To make this comparable to the manual word count estimates, which are derived from the 30 minutes of randomly sampled annotation, we calculate AWC per half hour.

#### Manual Word Count

We also calculated a manual count of speech in the children's environment. Manual word count is simply the number of intelligible words in our transcriptions of each child's recording. Speech that was too far or muffled to be intelligible, as well as speech from the target child and electronic speech (TV, radio, toys) are excluded from this count.

By using Adult Word Count and Manual Word Count, we hope to capture complementary estimates of the amount of speech children are exposed to. AWC is less accurate, but commonly used, and provides an estimate of the speech across the whole day. MWC, because it comes from human annotations, is the gold-standard for accurate speech estimates, but is only derived from 30 minutes of the recording.

### Interaction

#### Conversational Turn Count

One common metric of communicative interaction [e.g., @ganek2018; @magimairaj2022] is conversational turn count (or CTC), an automated measure generated by LENA [@xu2009]. Like AWC, a recent meta-analysis finds that CTC is associated with children's language outcomes [@wang2020]. After tagging vocalizations for speaker identity, the LENA algorithm looks for alternations between adult and target child speech in close temporal proximity (within 5 seconds). This can erroneously include non-contingent interactions (e.g., mom talking to dad while the infant babbles to herself nearby), and therefore inflate the count especially for younger ages and in houses with multiple children [@ferjanramirez2021]. Still, this measure correlates moderately well with manually-coded conversational turns [@busch2018; @ganek2018], and because participants in our sample are matched on both age and number of siblings, CTC overestimation should not be biased towards either group.

#### Proportion of Child-Directed Speech

Our other measure of interaction is the proportion of utterances that are child-directed, derived from the manual annotations. Each proportion was calculated as the number of utterances (produced by someone *other* than the target child) tagged with a child addressee out of the total number of utterances.

### Linguistic Features

#### Type-Token Ratio

As in previous work [e.g., @templin1957; @pancsofar2006; @montag2018], we calculated the lexical diversity of the input by dividing the number of unique words by the total number of words (i.e., the type-token ratio). Because the type-token ratio changes as a function of the size of the language sample [@richards1987; @montag2018], we first standardized the sample length by cutting children's input (from the manual annotations) in each recording into 100-word bins. We then calculated the type-token ratio within each of these bins by dividing the number of unique words in each bin by the number of total words (\~100) and then averaged the type-token ratio across bins for each child.

#### MLU

We also analyzed the syntactic complexity of children's language input, approximated as mean utterance length in morphemes. Both type-token ratio and mean length of utterance in speech to infants are consistent within individual caretakers, in and out of lab settings [@stevenson1986]. Each utterance was tokenized into morphemes using the 'morphemepiece' R package [@bratt2022]. We then calculated the mean length of utterance (number of morphemes) per speaker in each audio recording. We manually checked utterance length in a random subset of 10% of the utterances (n = `r nrow(MLU_agreement$subjects)`), which yielded a intra-class correlation coefficient of `r MLU_agreement$value` agreement with the udpipe approach (*p* `r printp(MLU_agreement$p.value)`), indicating high consistency.

### Conceptual Features

Our analysis of the conceptual features aims to measure whether the extent to which language input centers around the *"here and now"*: things that are currently present or occurring that a child may attend to in real time. Prior work has quantified such *here-and-now*ness by counting object presence co-occurring with a related noun label [e.g., @osina2013; @ganea2013; @moore1994; @harris1986]. The audio format of our data make it difficult to ascertain object presence, so instead of object displacement, we approximate *here-and-now*ness using lexical and morphosyntactic properties of the input.

#### Proportion of temporally displaced verbs

We examined the displacement of events discussed in children's linguistic environment, via properties of the verbs in their input. Notably, we are attempting to highlight semantic features of the language environment; however, given the constraints of large-scale textual analysis, we are categorizing utterances based on a combination of closely related syntactic and morphological features of verbs, since these contain some time information in their surface forms. We assigned each utterance a **temporality** value: utterances tagged *displaced* describe events that take place in the past, future, or irrealis space, while utterances tagged *present* describe current, ongoing events. This coding scheme roughly aligns with both the temporal displacement and future hypothetical categories in [@grimminger2020; see also: @lucariello1987; @hudson2002]. To do this, we used the udpipe package [@wijffels2023] to tag the transcriptions with parts of speech and other lexical features, such as tense, number agreement, or case inflection. To be marked as present, a verb either had to be marked with both present tense and indicative mood, or appear in the gerund form with no marked tense (e.g. *you talking to Papa?*). Features that could mark an utterance as displaced included past tense, presence of a modal, presence of *if*, or presence of *gonna*/*going to*, *have to*, *wanna*/*want to*, or *gotta*/*got to*, since these typically indicate future events, belief states and desires, rather than real-time events. In the case of utterances with multiple verbs, we selected the features from the first verb or auxiliary, as a proxy for hierarchical dominance. A small number of utterances in our corpus were left *uncategorized* (n = `r n_uncat`/`r nrow(verbs_only)`), either because they were fragments or because the automated parser failed to tag any of the relevant features. We manually checked verb temporality in a random subset of 10% of the utterances (n = `r nrow(manually_coded_displacement_subset)`); human judgments of event temporality aligned with the automated tense tagger `r round(displacement_agreement,2) * 100`%, indicating reasonably high reliability of this measure.

#### Proportion of highly visual words

In addition to this general measures of decontextualized language, we include one measure that is uniquely decontextualized for blind children: the proportion of words in the input with referents that are highly and exclusively visual. We categorize the perceptual modalities of words' referents using the Lancaster Sensorimotor Norms, ratings from typically-sighted adults about the extent to which a word evokes a visual/tactile/auditory/etc. experience [@lynott2020]. Words with higher ratings in a given modality are more strongly associated with perceptual experience in that modality. A word's dominant perceptual modality is the modality which received the highest mean rating. We tweak this categorization in two ways: words which received low ratings (\< 3.5/5) across all modalities were re-categorized as *amodal*, and words whose ratings were distributed across modalities (perceptual exclusivity \< 0.5/1) were re-categorized as *multimodal*. Using this system, each of the content words in children's input were categorized into their primary perceptual modality. For each child, we extracted the proportion of exclusively "visual" words in their language environment.

# Results

## Measuring Properties of Language Input

Our study assesses whether language input to blind children is different from the language input to sighted children, along the dimensions of quantity, interaction, linguistic properties, and conceptual properties. We test for group differences using paired t-tests or non-parametric Wilcoxon signed rank tests, when a Shapiro-Wilks test indicates that the variable is not normally distributed. Because this analysis involves multiple tests against the null hypothesis (*that there is no difference in the language input to blind vs. sighted kids*), we use the Benjamini-Hochberg correction [@benjamini1995] to control false discovery rate (Q = .05) for each set of analyses (quantity, interaction, linguistic, conceptual). Because each dimension's analysis consists of two statistical tests, our Benjamini-Hochberg critical values were *p* \< `r 1/2 * .05` for the smaller *p* value and *p* \< `r 2/2 * .05` for the larger *p* value.The results of these analyses are summarized in Table \@ref(tab:ps).

### Language Input Quantity

```{r compare-AWC}
AWC_check <- shapiro.test(LENA_counts$AWC_stand)
compare_AWC <- wilcox.test(LENA_counts$AWC_stand ~ LENA_counts$group, paired=TRUE)
AWC_stats <- LENA_counts %>%
  group_by(group) %>%
  summarise(min = min(AWC_stand),
            max = max(AWC_stand),
            mean = mean(AWC_stand))

MWC_check<-shapiro.test(manual_word_tokens$tokens)
compare_MWC <- t.test(manual_word_tokens$tokens ~ manual_word_tokens$group, paired=TRUE)
MWC_stats <- manual_word_tokens %>%
  group_by(group) %>%
  summarise(min = min(tokens),
            max = max(tokens),
            mean = mean(tokens))
```

We first compare the quantity of language input to blind and sighted children using two measures of the number of words in their environment: LENA's automated Adult Word Count and Manual Word Count. Shapiro-Wilks tests indicated that both of these variables were normally distributed (*p*s \> .05). 

Turning first to LENA's automated measure, a two-sample t-test shows that despite wide variability in the number of words children hear (Range: `r round((AWC_stats %>% filter(group=="VI"))$min)`--`r round((AWC_stats %>% filter(group=="VI"))$max)` words~blind~, `r round((AWC_stats %>% filter(group=="TD"))$min)`--`r round((AWC_stats %>% filter(group=="TD"))$max)` words~sighted~), blind and sighted children do not differ in language input quantity (*t*(`r compare_AWC$parameter`) = `r compare_AWC$statistic`, *p* `r printp(compare_AWC$p.value, add_equals=TRUE)`). If we instead measure this using word counts from the transcriptions of the audio recordings, we find parallel results: blind and sighted children do not differ in language input quantity (*t*(`r compare_MWC$parameter`) = `r compare_MWC$statistic`, *p* `r printp(compare_MWC$p.value, add_equals=TRUE)`); see Figure \@ref(fig:quantity-plots).

```{r quantity-plots, fig.cap="Comparing LENA-generated adult word counts (left) and transcription-based word counts in the input of blind and sighted children. Each dot represents the estimated number of words in one child's recording.", fig.width=6,fig.height=3}
AWC_plot <-
  ggplot(data = LENA_counts, aes(
    x = group,
    y = AWC_stand,
    color = group,
    fill = group)) +
  geom_violinhalf(alpha = .5, trim = FALSE, flip=1) +
    geom_line(aes(group = pair), color = "grey10", alpha = .1) +
  theme_classic() +
  labs(y = "Adult Word Count", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted")) +
  coord_cartesian(ylim = c(0, (max(LENA_counts$AWC_stand * 1.1)))) +
  theme(text = element_text(size = 10))

manual_word_count_plot <-  ggplot(data = manual_word_tokens, aes(
    x = group,
    y = tokens,
    color = group,
    fill = group)) +
  geom_violinhalf(alpha = .5, trim = FALSE, flip=1) +
    geom_line(aes(group = pair), color = "grey10", alpha = .1) +
  theme_classic() +
  labs(y = "Manual Word Counts", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted")) +
  coord_cartesian(ylim = c(0, (max(manual_word_tokens$tokens * 1.1)))) +
  theme(text = element_text(size = 10))

cowplot::plot_grid(AWC_plot, manual_word_count_plot,rel_widths = c(1,1),nrow = 1)
```

### Interaction

```{r compare-interaction}
CTC_normality_check <- shapiro.test(LENA_counts$CTC_stand)
compare_CTC <- wilcox.test(LENA_counts$CTC_stand ~ LENA_counts$group, paired=TRUE)
CTC_stats <- LENA_counts %>%
  group_by(group) %>%
  summarise(min = min(CTC_stand),
            max = max(CTC_stand),
            mean = mean(CTC_stand))

CDS_normality_check <- shapiro.test(xds_props_wide$prop_CDS)
compare_CDS <- t.test(xds_props_wide$prop_CDS ~ xds_props_wide$group, paired=TRUE)
CDS_stats <- xds_props_wide %>%
  group_by(group) %>%
  summarise(min = min(prop_CDS),
            max = max(prop_CDS),
            mean = mean(prop_CDS))
```

Next, we ask whether blind and sighted groups differ in the amount of interaction with the child, by comparing the proportion of child-directed speech and the number of conversational turns. Both measures were normally distributed (Prop. CDS: W = `r CDS_normality_check$statistic`, *p* `r papaja::printp(CDS_normality_check$statistic,add_equals=TRUE)`; CTC: W = `r CTC_normality_check$statistic`, *p* `r papaja::printp(CTC_normality_check$statistic,add_equals=TRUE)`). Paired t-tests revealed no significant difference in the proportion of child-directed speech (*t* = `r compare_CDS$statistic`, *p* `r printp(compare_CDS$p.value, add_equals=TRUE)`) or in conversational turn counts to blind children versus to sighted children .

```{r interaction-plots, fig.cap="Comparing LENA-generated conversational turn counts (left) and proportion of utterances in child-directed speech (center). Each dot represents one child's recording. The full breakdown by addressee is shown in the rightmost panel.", fig.width=7, fig.height=2.5}
CTC_plot <-
  ggplot(data = LENA_counts, aes(
    x = group,
    y = CTC_stand,
    color = group,
    fill = group)) +
  geom_violinhalf(alpha = .5, trim = FALSE, flip=1) +
    geom_line(aes(group = pair), color = "grey10", alpha = .1) +
  theme_classic() +
  labs(y = "Conversational Turn Count", x = NULL) +
  theme(
    legend.title = element_blank(),
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted")) +
  coord_cartesian(ylim = c(0, (max(LENA_counts$CTC_stand * 1.1)))) +
  theme(text=element_text(size=10))

CDS_prop_plot <- xds_props %>%
  filter(addressee == "CDS") %>%
  ggplot(aes(
    x = group,
    y = prop,
    color = group,
    fill = group)) +
  geom_violinhalf(alpha = .5, trim = FALSE, flip=1) +
    geom_line(aes(group = pair), color = "grey10", alpha = .1) +
  theme_classic() +
  labs(y = "Proportion of\nChild-Directed Speech", x = NULL) +
  theme(
    legend.title = element_blank(),
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted")) +
  coord_cartesian(ylim = c(0, 1)) +
  theme(text=element_text(size=10))

addressee_props_plot <- xds_props %>% filter(addressee!="TDS")%>%
  group_by(group, addressee) %>%
  summarise(prop = mean(prop,na.rm=TRUE)) %>%
  ggplot(aes(x=group,y=prop,fill=factor(addressee,levels=c("UDS","TDS","PDS","ODS","BDS","ADS","CDS")))) +
  geom_bar(stat="identity") +
  scale_fill_manual(name = "Addressee", breaks = c("CDS","ADS","BDS","ODS","PDS","UDS"),
                    labels = c("Child","Adult","Child & Adult", "Other", "Pet", "unknown"),
                    values = c("#FF70AE","#FFB870","#FFF170", "#BCE784","#ade7f5","#b16fff")
                    ) +
    geom_text(aes(label=case_when(prop>.05~round(prop,2))),
            position=position_stack(vjust=0.5)) +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted"))+
  theme_classic() +
  theme(text=element_text(size=10),
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("Proportion of Utterances\nBy Addressee")+
  xlab(NULL)

cowplot::plot_grid(CTC_plot, CDS_prop_plot, addressee_props_plot, rel_widths = c(1,1.05, 1.5),nrow=1)
```

### Linguistic Features

```{r linguistic-plots, fig.cap="Comparing linguistic features: Mean length of utterance (left); each dot represents one speaker. Type-token ratio (right). Each dot represents one child's recording.", fig.height=2, fig.width=4.5}
TTR_plot <-  ggplot(data = (TTR_calculations), aes(
    x = group,
    y = mean_ttr,
    color = group,
    fill = group)) +
  geom_violinhalf(alpha = .5, trim = FALSE, flip=1) +
    geom_line(aes(group = pair), color = "grey10", alpha = .1) +
  theme_classic() +
  labs(y = "Type-Token Ratio:\nUnique Words / Total Words", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Typically-\nDeveloping", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted")) +
  coord_cartesian(ylim = c(0, (max(TTR_calculations$mean_ttr * 1.1)))) +
  theme(text = element_text(size = 10))

TTR_normality_check <- shapiro.test(TTR_calculations$mean_ttr)
TTR_test <- t.test(TTR_calculations$mean_ttr~TTR_calculations$group, paired=TRUE)

MLU_plot <-
  ggplot(data = (MLUs %>% filter(MLU>0)), aes(
    x = group,
    y = MLU,
    color = group,
    fill = group)) +
  geom_violinhalf(alpha = .5, trim = FALSE, flip=1) +
  geom_line(aes(group = pair), color = "grey10", alpha = .1) +
  theme_classic() +
  labs(y = "Mean Length of Utterance\n
  (morphemes)", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none") +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "TD" = "Sighted",
      "VI" = "Blind")) +
  coord_cartesian(ylim = c(0, (max(MLUs$MLU * 1.1)))) +
  theme(text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1))

MLU_stats <- MLUs %>%
  group_by(group) %>%
  summarise(mean=mean(MLU),
            SE = std.error(MLU))
MLU_normality_check <- shapiro.test(MLUs$MLU)
MLU_compare <- t.test(MLUs$MLU ~ MLUs$group, paired=TRUE)

cowplot::plot_grid(MLU_plot, TTR_plot)
```

For linguistic features, we measure type-token ratio and mean length of utterance, two variables derived from the manual annotations. Because these variables met the normality assumption (TTR: W = `r TTR_normality_check$statistic`, *p* `r papaja::printp(TTR_normality_check$statistic,add_equals=TRUE)`; MLU: (W = `r MLU_normality_check$statistic`, *p* `r papaja::printp(MLU_normality_check$statistic,add_equals=TRUE)`)), we performed paired t-tests. Both variables differed across groups: blind children had a significantly higher type-token ratio (*t*(`r TTR_test$parameter`) = `r TTR_test$statistic`, *p* `r papaja::printp(TTR_test$p.value, add_equals=TRUE)`), such that per 100 words, blind children on average hear `r printnum(abs(TTR_test$estimate * 100))` more unique words than sighted children. Blind children also heard significantly longer MLU than to their sighted peers (*t*(`r MLU_compare$parameter`) = `r MLU_compare$statistic`, *p* = `r printp(MLU_compare$p.value)`), such that utterances in blind children's input were `r printnum(abs(MLU_compare$estimate))` morphemes longer; see Figure \@ref(fig:linguistic-plots)).

### Conceptual Features

```{r sensory-modality}

visual_plot <- ggplot(data = sensory_props_wide, aes(
    x = group,
    y = prop_Visual,
    color = group,
    fill = group)) +
    geom_line(aes(group = pair), color = "grey10", alpha = .1) +
  geom_violinhalf(alpha = .5, trim = FALSE, flip=1) +
  theme_classic() +
  labs(y = "Proportion of\nHighly Visual Words", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none") +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "TD" = "Sighted",
      "VI" = "Blind")) +
  coord_cartesian(ylim = c(0, .5)) +
  theme(text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1))

sensory_props_plot <- sensory_props %>%
  group_by(group,Modality) %>%
  summarise(prop = mean(prop,na.rm=TRUE)) %>%
  ggplot(aes(fill = factor(Modality,levels=c("Amodal","Olfactory","Gustatory","Haptic","Interoceptive","Auditory","Visual","Multimodal")),
             y= prop,
             x = group)) +
  geom_bar(position = "stack", stat = "identity")  +
  geom_text(aes(label=case_when(prop>.04~round(prop,2))),
            position=position_stack(vjust=0.5)) +
  xlab("Group") +
  ylab ("Proportion of words\nby sensory modality") +
  theme_classic()+
  theme(text=element_text(size=10),
    axis.text.x = element_text(angle = 45, hjust = 1)
        ) +
  scale_fill_manual(name="Modality",
                    breaks=c("Multimodal","Visual","Auditory","Interoceptive","Haptic","Gustatory","Olfactory", "Amodal"),
                    values=c("#FF70AE","#EF233C","#FFB870","#FFF170", "#BCE784","#ade7f5","#b16fff","#A5ACA5"))+
  scale_x_discrete(labels=c("VI" = "Blind", "TD" = "Sighted")) +
  xlab(NULL)
# then do this same graph, but by target child only   

visual_normality <- shapiro.test(sensory_props_wide$prop_Visual)
visual_test<- wilcox.test(sensory_props_wide$prop_Visual ~ sensory_props_wide$group, paired=TRUE)
```

```{r verb-tense}

hereandnow_plot <- ggplot(data = temporality_props_wide, aes(x = group,
    y = prop_displaced,
    color = group,
    fill = group)) +
    geom_line(aes(group = pair), color = "grey10", alpha = .1) +
  geom_violinhalf(alpha = .5, trim = FALSE, flip=1) +
  theme_classic() +
  labs(y = "Proportion of\nDisplaced Verbs", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none") +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "TD" = "Sighted",
      "VI" = "Blind")) +
  coord_cartesian(ylim = c(0, 1)) +
  theme(text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1))

temporality_props_plot <- temporality_props %>%
  mutate(group=as.factor(str_sub(VIHI_ID,1,2))) %>%
  group_by(group, verb_temporality) %>%
  summarise(prop = mean(prop, na.rm = TRUE)) %>%
  ggplot(aes(fill = factor(verb_temporality, levels=c("uncategorized","displaced","present")),
                                 y = prop,
                                 x = group)) +
  geom_bar(position = "stack", stat = "identity")  +
  ylab ("Proportion of utterances\nby verb temporality") +
   geom_text(aes(label=round(prop,2)),
            position=position_stack(vjust=0.5)) +
  theme_classic() +
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(name="Verb Tense",
                    labels=c("uncategorized", "displaced", "present"),
                    values=c("#A5ACA5","#FFF170","#BCE784"))+
  scale_x_discrete(labels =                                                   c("VI" = "Blind", "TD" = "Sighted")) +
  xlab(NULL)

temporality_normal<- shapiro.test(temporality_props_wide$prop_displaced)
temporality_test <- t.test(temporality_props_wide$prop_displaced ~ temporality_props_wide$group, paired=TRUE)
```

```{r conceptual-plots, fig.cap="Left col: Comparing proportion of temporally displaced verbs (top) and proportion of highly visual words (bottom). Each dot represents the one child's recording, with black dot and whiskers showing means and standard errors. Right col: Full distribution of verb types (top)  and sensory modality (bottom) by group, collapsing across participants.", fig.height=8, fig.width=7}
cowplot::plot_grid(hereandnow_plot,temporality_props_plot,visual_plot,sensory_props_plot,cols = 2, rel_widths = c(1.5,2))
```

Lastly, we compared two measures of the conceptual features of language input: the proportion of temporally displaced verbs and the proportion of highly visual words. Because the proportion of displaced verbs follows a normal distribution (W = `r temporality_normal$statistic`, *p* `r papaja::printp(temporality_normal$statistic,add_equals=TRUE)`), we tested this measure with a paired t-test and found that blind children hear `r printnum(abs(temporality_test$estimate))` more displaced verbs than sighted children (*t*(`r temporality_test$parameter`) = `r temporality_test$statistic`, *p* `r papaja::printp(temporality_test$p.value, add_equals=TRUE)`). For the proportion of highly visual words, a Shapiro-Wilks test showed that this variable was not normally distributed (W = `r visual_normality$statistic`, *p* `r papaja::printp(visual_normality$statistic,add_equals=TRUE)`). A paired Wilcoxon test found no significant difference across groups in the proportion of highly visual words (*W*(`r visual_test$parameter`) = `r visual_test$statistic`, *p* `r printp(visual_test$p.value, add_equals=TRUE)`).

```{r ps, results='asis'}

get_group_means <- function(data, column, subset, unit = "",adjust=1,digits=2) {
  filtered_data <- data %>%
    filter({{ subset }})
  
  mean_value <- mean(filtered_data[[column]], na.rm=TRUE)*adjust
  mean_string <- glue::glue("{round(mean_value,digits)} {unit}")
  return(mean_string)
}

vars <-
  c(
    "Adult Word Count",
    "Manual Word Count",
    "Prop. Child-Directed Speech",
    "Conversational Turn Count",
    "Type-Token Ratio",
    "Mean Length of Utterance",
    "Prop. Displaced",
    "Prop. Visual"
  )
  `Portion of Recording` <- c("Whole day", "Random", "Whole day", "Random", "Random + High Volume",
                             "Random + High Volume", "Random + High Volume", "Random + High Volume")
  Description <- c("Estimated number of words in recording categorized as nearby adult speech by LENA algorithm",
                  "Number of word tokens from speakers other than target child",
                  "Count of temporally close switches between adult and target-child vocalizations, divided by recording length",
                  "Number of utterances tagged with child addressee out of total number of utterances, from speakers other than target child",
                  "Average of the type-token ratios (number of unique words divided by number of total words) for each of the 100-word bins in their sample",
                  "Average number of morphemes per utterance",
                  "Proportion of verbs that refer to past, future, or hypothetical events",
                  "Proportion of words in the input with high visual association ratings and low ratings for other perceptual modalities")

tests <-
  c(
    "Paired Wilcoxon test",
    "Paired t-test",
    "Paired Wilcoxon test",
    "Paired t-test",
    "Paired t-test",
    "Paired t-test",
    "Paired t-test",
    "Paired Wilcoxon test"
  )
MeanBlind <-
  c(
    get_group_means(LENA_counts, "AWC_stand", group == "VI", unit = "words/hour",adjust=2, digits=0),
    get_group_means(manual_word_tokens, "tokens", group == "VI", unit = "words/hour", adjust=2, digits=0),
    get_group_means(LENA_counts, "CTC_stand", group == "VI", unit = "turns/hour",adjust=2, digits=0),
    get_group_means(xds_props_wide, "prop_CDS", group == "VI"),
    get_group_means(TTR_calculations, "mean_ttr", group == "VI", unit = "words/hour"),
    get_group_means(MLUs, "MLU", group == "VI", unit = "morphemes"),
    get_group_means(temporality_props_wide, "prop_displaced", group == "VI"),
    get_group_means(sensory_props_wide, "prop_Visual", group == "VI")
  )
MeanSighted <-
  c(
    get_group_means(LENA_counts, "AWC_stand", group == "TD", unit = "words/hour",adjust=2, digits=0),
    get_group_means(manual_word_tokens, "tokens", group == "TD", unit = "words/hour", adjust=2, digits=0),
    get_group_means(LENA_counts, "CTC_stand", group == "TD", unit = "turns/hour",adjust=2, digits=0),
    get_group_means(xds_props_wide, "prop_CDS", group == "TD"),
    get_group_means(TTR_calculations, "mean_ttr", group == "TD", unit = "words/hour"),
    get_group_means(MLUs, "MLU", group == "TD", unit = "morphemes"),
    get_group_means(temporality_props_wide, "prop_displaced", group == "TD"),
    get_group_means(sensory_props_wide, "prop_Visual", group == "TD")
  )
raw_ps <-
  c(
    compare_AWC$p.value,
    compare_MWC$p.value,
    compare_CDS$p.value,
    compare_CTC$p.value,
    TTR_test$p.value,
    MLU_compare$p.value,
    temporality_test$p.value,
    visual_test$p.value
  )
ps_df <- bind_cols(vars, `Portion of Recording`, Description, tests, MeanBlind, MeanSighted, raw_ps) %>%
  dplyr::rename(
    "Variable" = `...1`,
    "Portion of Recording" = `...2`,
    "Description" = `...3`,
        "Test" = `...4`,
        "Mean Blind" = `...5`,
    "Mean Sighted" = `...6`,
    "p value" = `...7`
  ) %>%
  mutate(`p value` = case_when(`p value` < .05 ~ paste0(printp(`p value`), "*"),
                               TRUE ~ printp(`p value`)))
kable(ps_df, caption = "Summary of analyses over language input variables.", align = c("l", "l", "l", "l", "l", "l","c")) %>%
  kable_styling(font_size = 9) %>%
  column_spec(1, width = ".82in") %>%
    column_spec(2, width = ".76in") %>%
  column_spec(3, width = "2in") %>%
    column_spec(4, width = ".78in") %>%
    column_spec(5, width = ".7in") %>%
  column_spec(6, width = ".7in") %>%
  column_spec(7, width = ".43in") 

```


```{r predicting-outcomes}

# # only AWC has a measurable association with CVC, and it doesnt vary by group
# model_AWC <- lm(CVC ~ LENA_age_in_days + group *AWC, data=lotta_data)
# model_CDS <- lm(CVC ~ LENA_age_in_days + group* prop_CDS, data=lotta_data)
# model_TTR <- lm(CVC ~ LENA_age_in_days + group*  mean_ttr, data=lotta_data)
# model_MLU <- lm(CVC ~ LENA_age_in_days+ group * MLU, data=lotta_data)
# model_displaced <- lm(CVC ~ LENA_age_in_days * group+ prop_displaced, data=lotta_data)
# model_visual <- lm(CVC ~ LENA_age_in_days + group* prop_Visual, data=lotta_data)

```


# Discussion

This study, which contains more blind participants than prior research alongside a carefully peer-matched sighted sample, measured language input to young blind children and their sighted peers, using the LENA audio recorder. We found that along the dimensions of quantity and interaction, parents talk similarly to blind and sighted children, with differences in linguistic and conceptual content of the input. We discuss each of these results further below.

## Quantity

Across two measures of language input quantity, one estimated from the full sixteen hour recording (Adult Word Count) and one precisely measured from a 30-minute window of that day (Manual Word Count), blind and sighted children were exposed to similar amounts of speech in the home. Quantity was highly variable *within* groups, but we found no evidence for *between* group differences in input quantity. This runs counter to two folk accounts of language input to blind children: 1) that sighted parents of blind children might talk *less* because they don't share visual common ground with their children; 2) that parents of blind children might talk *more* to compensate for their children's lack of visual input. Instead, we find a similar quantity of speech across groups.

## Interaction

We quantified interaction in two ways: through the LENA-estimated conversational turn count and through the proportion of child-directed speech in our manual annotations. Again, we found no differences across groups in the amount of parent-child interaction. This finding contrasts with previous research; other studies report *less* interaction in dyads where the child is blind [@rowland1984; @perez-pereira2001; , @moore1994; @kekelis1984; @preisler1991; @andersen1993; @grumi2021]. Using a non-visual sampling method (i.e., our audio recordings) might provide a different, more naturalistic perspective on parent-child interactions, particularly in this population. For one thing, many prior studies [e.g., @kekelis1984; @preisler1991; @moore1994; @perez-pereira2001] involve videorecordings in the child's home, with the researcher present. Like other young children, blind children distinguish between familiar individuals and strangers, and react with trepidation to the presence of a stranger; for blind children, this reaction may involve "quieting", wherein children cease speaking or vocalizing when they hear a new voice in the home [@mcrae2002; @fraiberg1975]. By having a researcher present during the recordings[^3] , prior research may have artificially suppressed blind children's initiation of interactions. Even naturalistic observer-free videorecordings appear to inflate aspects of parental input, relative to daylong audio recordings [@bergelson2019]. Together, these factors could explain why past parent-child interaction research finds that blind children initiate fewer interactions [@andersen1993; @kekelis1984; @dote-kwan1995; @troster1992; @moore1994], that parents do most of the talking [@kekelis1984; @andersen1993], and that there is overall less interaction [@rowland1984; @nagayoshi2017; @rogers1984; @troster1992].

[^3]: Fraiberg (1975) writes "these fear and avoidance behaviors appear even though the observer, a twice-monthly visitor, is not, strictly speaking, a stranger." (pg. 323).

Additionally, a common focus in earlier interaction literature is to measure visual cues of interaction, such as shared gaze or attentiveness to facial expressions [@preisler1991; @baird1997; @rogers1984]. We can't help but wonder: are visual markers of social interaction the right yardstick to measure blind children against? In line with @macleod2023, perhaps the field should move away from sighted indicators of interaction "quality", and instead situate blind children's interactions within their own developmental niche, one that may be better captured with auditory- or tactile-focused measures.

## Linguistic Features

Along the linguistic dimension, we measured type-token ratio and mean length of utterance. Parents of children with disabilities [including parents of blind children! e.g., @familyconnect; @chernyak] are often advised to use shorter, simpler sentences with their children; correspondingly, previous work finds that parents of children with disabilities tend to find that parents *do* use shorter, simpler utterances [e.g., Down syndrome, @lorang2020; hearing loss, @dirks2020]. We therefore expected to observe shorter utterances and less lexical diversity. Instead, we found that blind children heard on average `r printnum(abs(TTR_test$estimate * 100))` more unique words per hundred words and `r printnum(abs(MLU_compare$estimate))` more morphemes per utterance [which is similar in magnitude to @dirks2020's MLU difference between mothers of typically-hearing vs. deaf children, and three times larger than @hoff2003's MLU difference between low- and high-SES mothers]. These results suggest that blind children are exposed to more lexically and morphosyntactically complex speech. Returning to the potential impact on children, evidence suggests that (contrary to the advice often given to parents), longer, more complex utterances are associated with better child language outcomes in both typically-developing children [@hoff2002] and children with cognitive differences [@sandbank2016]. And similarly, higher lexical diversity is associated with larger vocabulary [@rowe2012; @anderson2021; @huttenlocher2010; @hsu2017; @weizman2001]. If these patterns are the same for blind children, this additional linguistic complexity may pose helpful in the absence of visual input.

## Conceptual Features

Although there are many potential ways to measure the conceptual features of language, we chose to capture *here-and-now*-ness by measuring the proportion of temporally displaced verbs and the proportion of highly visual words. We found that blind children heard more temporally displaced verbs. Though blind and sighted participants were exposed to a similar proportion of highly visual words, the referents of these words are by definition, inaccessible to the blind participants. Taken together, our conceptual results suggest that blind children's input is *less* focused on their *here-and-now*.

The extent to which blind children's language input is centered on the *here-and-now* has been contested in the literature [@urwin1984; @moore1994; @andersen1993; @campbell2003; @kekelis1984]. This aspect of language input is of particular interest because early reports suggest that blind children's own use of decontextualized language develops later than sighted children's[@urwin1984; @bigelow1990]. Could this be related to an absence of decontextualized language in the input? Our sample says no: we find that blind children's input contains *more* decontextualized language. Because children have less access to immediate visual cues, caregivers might more frequently refer to past or future events to engage with their child. To illustrate, while riding on a train, instead of describing the scenery passing outside the window, parents may choose to talk about what happened earlier in the day or their plans upon home. Without further information about the social and perceptual context, it is difficult to determine the communicative function of the differences we find in conceptual features we find or how they might explain differences in children's decontextualized language use. As more dense annotation becomes available, we can explore the social and environmental contexts of conceptual information as it unfolds across discourse.

We wish to highlight again how much variability there is *within* groups and how much consistency there is *between* groups. One could imagine a world in which the language environments of blind and sighted children are radically different from each other. Our data do not support that hypothesis. Rather, we find similarity in quantity and interaction, alongside modest differences in linguistic and conceptual properties. This is worth emphasizing and re-emphasizing: across developmental contexts, including, as we show here, visual experience, children's language input is resoundingly similar [@bergelson2022a].

## Connecting to Language Outcomes

Blind children's language input in this sample looks more linguistically and conceptually complex than input to sighted infants--what consequences might this have for language acquisition? Unfortunately, given the varied age range of this sample, we are underpowered to measure how these dimensions of language input affect children's language learning, but prior literature allows us to speculate. We explore two possibilities: First, if input effects pattern similarly for blind and sighted children, we would expect blind and sighted children alike to benefit from more input [cite], more interactive input [cite], more linguistically complex input [cite], and more conceptually complex input [cite]. If this is the case, then receiving more complex input might help blind children to learn linguistic structure. In turn, understanding linguistic structure [@gleitman1990] may support blind children's acquisition of word meanings in the absence of vision. Alternatively, what if language input affects acquisition *differently* for blind children than it does for sighted children? Is it possible that blind children actually need *less* complex language, and that differences in language input are responsible for early vocabulary delays? 

To show our cards, we are inclined to believe the former account: that blind children benefit from receiving more complex language input. Language regularly supports learning in the absence of direct sensory perception (e.g., reading a book about mythical creatures). Given the language skills of blind adults [@roder2000; @roder2003; @loiotile2020], it seems likely that language is a source of meaning for blind individuals as well [@lewis2019; @vanparidon2021; @campbell2022]. Testing each of these predictions awaits further research.

In either case, if properties of language input do influence blind children's language outcomes, attempting to train parents to talk differently may be unfruitful. Interventions where parents are trained to talk differently to their children often show short-term success [@roberts2019] but fail to change parental speech patterns in the long term [e.g., @mcgillion2017; @suskind2016]. 

# Conclusion

In summary, our study compared language input in homes of 15 blind and 15 sighted infants. We found that both groups received similar quantities of adult speech and had similar levels of interaction. However, blind children were exposed to longer utterances and more decontextualized language, suggesting that they are being exposed to a rich and complex linguistic environment that differs from the language input of sighted children. Our study does not imply that parents should change their communication styles, but rather highlights the unique language experiences of blind children. Future research should investigate how these input differences impact the language development and cognitive abilities of blind and sighted children alike.

```{r trackdown}
# trackdown::update_file(file="input_quality_manuscript.Rmd",gpath="trackdown/input_quality",hide_code=TRUE, path_output = "input_quality_manuscript.pdf")
```

# References
