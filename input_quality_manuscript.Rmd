---
title             : "Language Input to Blind Infants/Toddlers"
shorttitle        : "Language Input to Blind Infants/Toddlers"


author: 
  - name          : "Erin Campbell"
    affiliation   : "1"
    corresponding : yes    
    email         : "erin.e.campbell@duke.edu"
    address       : "417 Chapel Drive, Box 90086, Durham, NC 27708"
  - name          : "Lillianna Righter"
    affiliation   : "1"
  - name          : "Elika Bergelson"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology & Neuroscience, Duke University, Durham, NC"

note: | 
  **Conflicts of Interest**: The authors have no conflicts of interest to report.
  **Funding**: This work was supported by the National Science Foundation CAREER grant (BCS-1844710) to EB and Graduate Research Fellowship (2019274952) to EC. 

keywords          : "keywords"
wordcount         : "X"

bibliography      : ["VI_InputQuality.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r libraries, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
library(ggplot2)
library(plotrix)
library(papaja)
library(stringr)
library(morphemepiece)
library(tidytext)
library(wordbankr)
library(reshape2)
library(profileR)
library(lubridate)
library(corrplot)

#lets create a function to convert things into percents (I stole this obvi)
percent <- function(x, digits = 1, format = "f", ...) {      # Create user-defined function
  paste0(formatC(x * 100, format = format, digits = digits, ...), "%")
}
seconds_to_length <- function(seconds, format = "f", ...) {      
  paste0(hour(seconds_to_period(seconds)), " hours ", minute(seconds_to_period(seconds)), " minutes")
}


preprocess <- TRUE
new_graphs <- TRUE

if (preprocess == TRUE) {source("input_quality_preprocessing.R")} else {
# quantity
LENA_counts <- read.csv("data/LENA/Automated/LENA_counts.csv")
# interactive
xds_props <- read.csv("data/LENA/Transcripts/Derived/xds_props.csv")
xds_props_wide <- read.csv("data/LENA/Transcripts/Derived/xds_props_wide.csv")
# linguistic
MLUs <- read.csv("data/LENA/Transcripts/Derived/MLUs.csv")
manual_word_TTR <- read.csv("data/LENA/Transcripts/Derived/manual_word_TTR.csv")
# conceptual
sensory_props <- read.csv("data/LENA/Transcripts/Derived/sensory_props.csv")
sensory_props_wide <- read.csv("data/LENA/Transcripts/Derived/sensory_props_wide.csv")
temporality_props <- read.csv("data/LENA/Transcripts/Derived/temporality_props.csv")
temporality_props_wide <- read.csv("data/LENA/Transcripts/Derived/temporality_props_wide.csv")
subj_CBOI_means <- read.csv("data/LENA/Transcripts/Derived/subj_CBOI_means.csv")
content_words_only <- read.csv("data/LENA/Transcripts/Derived/content_words_only.csv")}
# CDI
VIHI_CDI <- read.csv("data/CDI/Wordbank/VIHI_CDI.csv") %>% 
  dplyr::rename("CDI_age_in_days" = age)
```

```{r get-data}

participant_summary <- LENA_counts %>%
  mutate(Age_days = as.numeric(str_sub(VIHI_ID, 8, length(VIHI_ID))),
         Age_months = Age_days * 0.0328767) %>%
  group_by(group) %>%
  summarize(age = mean(Age_months,na.rm=TRUE),
            min_age = min(Age_months),
          max_age = max(Age_months),
          N=n())
            

```

# Introduction

The early language skills of blind children are highly variable [@campbell_inprep], with some blind children demonstrating age-appropriate vocabulary from the earliest stages of language learning [@landau1985; @bigelow_early_1987], while others experience large and persistent language delays [@CITE]. Canonically, blind adults become competent speakers of their language and are even reported to have faster language processing skills than their sighted peers [@roder_event-related_2000; @roder_semantic_2003]. The causes of this variability and the later ability to "catch up" remain poorly understood. In particular, the higher incidence of severe language delays in blind children yields questions about the process of language development in the absence of visual perception: what makes the language learning problem apparently more difficult for the blind child? There are multiple possible contributors, including characteristics of the child (e.g., visual acuity, comorbid conditions, gender) as well as characteristics of the environment (e.g., access to early intervention services; school setting; caretakers tailoring interactions to their child's sensory access). Here, we explore the characteristics of the language environment of blind children as it compares to the language environment of their sighted peers. In doing so, we begin to narrow down the role that visual input plays in language development, among all other factors.

Among both typically-developing children and children with developmental differences, language input is an important predictor of language outcomes [@gilkerson2018; @huttenlocher1991; @rowe2008; @anderson2021; @rowe2012; @anderson2021; @huttenlocher2010]. There are many ways to operationalize language input, that tend to be grouped into **quantity of language input** and **quality of language input**. Quantity of language input can be broadly construed as the number of words or utterances a child is exposed to. At a coarse level, children who are exposed to more speech [or sign, @watkins_deaf_1998] tend to have better language outcomes [@gilkerson2018; @huttenlocher1991; @rowe2008; @anderson2021]. However, if only the *amount* of language exposure mattered, then infants should be able to sit in front of the television all day and become fluent language users. Yet young children struggle to learn language from video [e.g., @roseberry2014].

Language input quality is perhaps even more important [@rowe2012; @hirsh-pasek2015], although it is somewhat trickier to devise what features constitute input quality. Rowe and Snow [@rowe2020] divide this space into three dimensions of language input quality: interactive features (e.g., parent responsiveness, speech directed *to* child vs. overheard; conversational turn-taking), linguistic features (e.g., lexical diversity, grammatical complexity), and conceptual features (e.g., topic diversity). These environmental features at various stages interact with the child's own cognitive, linguistic, and conceptual abilities.

In terms of linguistic quality, two common ways to quantify it are lexical diversity (often type/token ratio) and syntactic complexity. Lexical diversity of language input seems to exert different effects as children get older. In early infancy, children who are exposed to more repetitions at 7 months have higher vocabulary at age 2 [@newman2016]. This relationship later flips: toddlers who are exposed to greater diversity of words in their language input tend to have larger vocabulary scores [@rowe2012; @anderson2021; @huttenlocher2010; @hsu2017; @weizman2001]. Lexical diversity is intertwined with input quantity: parents who talk more also tend to provide more lexical diversity [@hoff2002]. Likewise, the diversity of syntactic constructions in parental language input is associated both with children's vocabulary growth and utterance structure diversity [@hoff2003; @naigles1998; @devilliers1985; @huttenlocher2010; @hadley2017; @huttenlocher2002].

One important feature of the language environment is the amount of interactivity in parent-child communication. Back-and-forth communicative exchanges (also known as conversational turns) between caregivers and children predict better language learning across infancy [@goldstein2008; @donnellan2020] and toddlerhood [@hirsh-pasek2015; @romeo2018], indicating that parents responding to their children's actions and utterances supports their learning. Another facet of interactivity is how adults attune to children's cues of attention and interest, like pointing or eye gaze. In infancy, words heard in contexts where the adult and child share joint attention are more likely to be learned [@lucca2017; @tomasello1986]. Parents' interaction with their child and the world around them interacts with the conceptual features of the language input.

Conceptual features of language input involve the relationship between conversational topics, referents in the world around them, and how children represent these things in their mind, depending on their cognitive level. For example, young infants are more likely to learn words when the object is perceptually salient, dominating their field of view [@yu2012]. Parents responding to a child's point and labeling the object of interest might boost learning in that instance. By contrast, *decontextualized* language use-- that is, talking about past and future events, or people and items that are not currently present in the environment-- may be beneficial at later stages of development [@rowe_decontextualized_2013]. In toddlerhood, parents' decontextualized language use predicts kindergarten vocabulary [@rowe2012], children's own decontextualized language use [@demir2015], and academic achievement in adolescence [@uccelli2019]. Decontextualized language may be helpful because it provides an opportunity to discuss a broader range of topics and reflects typical adult language usage, which is often abstract [@CITE]. It also provides the opportunity for more lexical and syntactic diversity, both of which are important features of linguistic input quality.

Lexical diversity is often calculated as a word type/token ratio, and syntactic complexity often uses mean length of utterance as a proxy. Lexical diversity of language input seems to exert different effects as children get older. In early infancy, children who are exposed to more repetitions (and therefore less lexical diversity) at 7 months have higher vocabulary at age 2 [@newman2016], which suggests that less lexical diversity may be beneficial in early stages of language learning. This relationship later flips: toddlers and preschool-aged children who are exposed to greater diversity of words in their language input tend to have larger vocabulary scores [@rowe2012; @anderson2021; @huttenlocher2010; @hsu2017; @weizman2001]. Lexical diversity is also intertwined with input quantity: parents who talk more also tend to provide more lexical diversity [@hoff2002], and thus these measures are difficult to untangle. Likewise, the diversity of syntactic constructions in parental language input is associated both with children's vocabulary growth and the structural diversity of their own productions [@hoff2003; @naigles1998; @devilliers1984; @huttenlocher2010; @hadley2017; @huttenlocher2002].

For blind children, their language input may constitute the brunt of available clues for learning in proportion to other parts of the environment [@campbell2022], starting from early on and as they grow up. In the absence of visual input, language is an important source of information about the world. Linguistic structure provides cues to word meaning that may be lost without visual cues. In our review so far, we have presented a pattern wherein the features of the input that are most helpful for language learning change over the course of children's development: early on, many of these cues require visual access (such as parental gaze, shared visual attention, pointing to remote object and the presence of salient objects in the visual field) and later become more abstract. This may be part of the reason why language delays are more common in blind children, but are often recovered in older blind children. It may take longer to gain enough environmental experience to make early language learning strides, but once they are able to use more abstract and linguistic features as cues, learning proceeds rapidly [@CITE or rephrase or take out this last sentence?]. All that said, we cannot assume that the *only* difference in the language learning experiences for blind and sighted the children is their access to visual experience. The language input itself may very well differ for blind children relative to sighted children, for a variety of reasons.

First, speakers regularly tailor input to communicate efficiently with the listener [@grice1975]. Parents are sensitive to their child's developmental level and tune language input accordingly [@snow1972; @vygotsky1978]. Child-directed speech is one example--whereby parents speak to young children with exaggerated prosody, slower speech rate, and increased vowel clarity [@fernald1989; @bernsteinratner1984], which appears to be helpful to the young language learner [@thiessen2005]. Parents show increased alignment (a tendency to re-use use the conversation partner's expressions) for younger children, that decreases as children get older [@yurovsky2016]. When interacting with infants and toddlers, parents repeat words more often than when interacting with older children or adults [@snow1972]. Communicative tailoring is also common in language input to children with disabilities, who tend to receive simplified, more directive language input, and less interactive input compared to typically-developing children [@dirks2019; @yoshinaga-itano2020].

In addition to tailoring communication to children's developmental level, speakers also adjust their conversation in accordance with the listener's sensory access [@gergle2004; @grigoroglou2016]. In a noisy environment, speakers will adapt the acoustic-phonetic features of their speech with the intent to make it easier for their interlocutor to understand them [@hazan_acoustic-phonetic_2011], which demonstrates sensitivity to the sensory conditions of their conversation partner, even temporary ones. When describing scenes, speakers aim to provide the information their listeners lack but avoiding redundant visual description [@ostarek2019; @grice1975]. During in-lab tasks with sighted participants, participants tailor their descriptions and requests by verbally providing visually-absent cues when an object is occluded to their partner [@jaraettinger2021; @hawkins2021; @rubio-fernandez2019]. These results suggest that adults and even infants [@senju2013; @ganea2018; @chiesa2015] can flexibly adapt communication to the visual and auditory abilities of their partner.

Curiously though, these results aren't borne out in the existing literature on interactions between blind infants and their sighted parents. We might expect parents to verbally compensate for missing visual input, resulting in parents providing more description of the child's environment. Instead, caregivers of blind children seem to restrict conversation to things that the blind child is currently engaged with, rather than attempt to redirect their attention to other stimuli [@andersen1993; @campbell2003; @kekelis1984]. In naturalistic settings, parents of blind children use *fewer* declaratives and *more* imperatives and requests for actions/labels than parents of sighted children, suggesting that children might be receiving less description than sighted children [@kekelis1984; @landau1985]. That said, we do see some evidence for parents adapting to their child's visual abilities. @tadic2013 and colleagues find that in a more structured book reading task, parents of blind children provide more descriptive utterances than parents of sighted children. Further, parents of blind children provide more tactile cues to initiate interactions or establish joint attention [@urwin1983; @preisler1991]. These mixed results suggest that parents of blind children might alter language input in some domains but not others.

Better understanding how perceptual and linguistic input interact and influence blind children's language outcomes is of great clinical and scientific importance. Based on our own interactions with participants' families in the present study, parents are looking for evidence-based guidance to help them support their children's language development. If properties of language input influence the likelihood of language delays among blind infants and toddlers [@campbell_inprep], then communicating this to families could help children reach their full potential. By contrast, if there is no relationship between language input properties and children's language outcomes, then perhaps language input is one less worry for caregivers. In the present study, we examine daylong recordings of naturalistic at-home language interactions between caregivers and their blind or sighted children in order to characterize their input. To understand whether parents speak differently to blind children than to sighted children, we first measure input along the dimensions of quantity (adult word count) and quality, split into interactivity (conversational turn counts, proportion of child-directed vs. adult-directed speech), conceptual features (topic diversity, tense, adjective typicality, sensory modality), and linguistic features (diversity measures: type/token ratio and mean length of utterance). We then link these features of language input to language outcomes, exploring whether the effects of parent language input on child language vary as a function of children's perceptual ability.

# Methods

## Participants

`r LENA_counts %>% filter(group=="VI") %>% nrow()` blind infants and their families participated in this study. Blind participants were recruited through opthamologist referral, preschools, early intervention programs, social media, and word of mouth. To be eligible for this study, participants had to be 6--30 months old, have no additional disabilities (developmental delays; intellectual disabilities, or hearing loss), and be exposed to $\geq$ 75% English at home. Given the wide age range of the study, to control for age, each blind participant was matched to a sighted partcipicant, based on age ($\pm$ 6 weeks), gender, maternal education ($\pm$ one education level: less than high school diploma, high school diploma, some college / Associate's, Bachelor's, graduate school), and number of siblings ($\pm$ 1 sibling). When more than one match was available, we prioritized matching the blind participants as closely as possible on each characteristics in the preceding order. Caregivers were asked to complete a demographics survey and the MacArthur-Bates Communicative Development Inventory [CDI; @fenson1994] within one week of the home language recording. See XXX for sample characteristics.

```{r participant-characteristics}

```


: Demographic characteristics of participants in the sample

## Recording Procedure

Eligible families were asked to complete two surveys and complete a daylong home language recording. For the recording portion of the study, caregivers of participating infants received a LENA wearable audio recorder [@ganek2016] and vest. They were instructed to place the recorder in the vest on the day of their scheduled recording and put the vest on their child from the time they woke up until the recorder automatically shut off after 16 hours (setting vest nearby during bath, nap, and car times). They were also instructed how to pause the recording at any time, but asked to keep these pauses to a minimum. Actual recording length ranged from `r seconds_to_length(min(LENA_counts$total_time_dur))` to `r seconds_to_length(max(LENA_counts$total_time_dur))` (`r seconds_to_length(mean(LENA_counts$total_time_dur, na.rm=TRUE))`).

## Processing

Audio recordings were first processed by LENA proprietary software, creating algorithmic measures such as conversational turn counts. Each recording was then run through an in-house automated sampler that selected 15- non-overlapping 5-minute segments, randomly distributed across the duration of the recording. The process output a codeable ELAN file (.eaf, @ELAN). Each segment consists of 2 core minutes of annotated time, with 2 minutes of listenable context marked out preceding the annotation clip and 1 minute of additional context following the annotation clip. Each file therefore contains 30 minutes of coded recording time and 75 minutes of total time listened (#isn't there one where that's not true??) Because these segments were sampled randomly, and not on a high-volubility measure such as conversational turns or adult speech density, the amount of time with codeable speech input varied for each recording. Indeed, across participants (FIND A WAY TO DO MATH WITH \# SEGMENTS THAT ARE SILENT) of the 2-minute coding segments contained no speech at all.

Once the randomly selected segments were annotated, we also chose to annotate 15 additional segments specifically for their high levels of speech. To select these segments of dense talk, we first conducted an automated analysis of the audio file using the voice type classifier for child-centered daylong recordings [@lavechin2020] which identified all human speech in the recording. The entire recording was then broken into 2-minute chunks marked out at zero-second timestamps (e.g. 00:02:00.000 to 00:04:00.000). Each of these chunks was then ranked highest to lowest by the total duration of speech contained within the boundaries. For our high volubility sample, we chose the highest-ranked 15 segments of each recording, excluding those that overlapped with already-coded random segments.

## Annotation

Trained annotators listened through each 2-minute segment plus its surrounding context and coded it using the Analyzing Child Language Experiences around the World (ACLEW) Daylong Audio Recording of Children's Linguistic Environments (DARCLE) annotation scheme [@soderstrom2021]. Prior to annotating lab data, annotators are trained on previously coded samples of child recordings and are required to reach 95% overall agreement with the gold standard version of the file for three different age ranges: 0-7 months, 8-18 months, and 19-36 months. For more information about this annotation scheme and the larger project, please see the ACLEW homepage (<https://sites.google.com/view/aclewdid/home>). Following the first pass, all files were by a highly-trained "superchecker" to ensure the consistency of annotations.

This annotation scheme is designed to capture both utterances by the target child and speech in the child's environment, including adults, other children, and pre-recorded electronic speech (e.g. toys, television, the radio). Annotators segment the duration of each utterance on a separate coding tier for each unique speaker (exceptions: all electronic speech is coded on the same tier, and some speakers who appear briefly in these files were not easily distinguishable from others by annotators naive to their identities, so they may be concatenated on the same tier). Speech by people other than the target child is transcribed using an adapted version of CHAT transcription style [@macwhinney2019], dubbed minCHAT for the ACLEW project [@soderstrom2021]. Because the majority of target children in the project are pre-lexical or phonetically immature, utterances produced by the target child are not transcribed.

Each utterance is coded for additional linguistic properties from a set of pre-determined categories. Target child utterances are coded for vocal maturity, lexical status, and multi-word status. Vocal maturity classifies utterances into the following categories: laughing; crying; canonical syllables that contain a consonant-like and vowel-like sound component, including both babbling and identifiable words; non-canonical syllables, which do not contain both consonant and vowel portions, or which do not transition between them in a speech-like way; and unsure, when the vocalization type is unclear. Each vocalization that contains canonical syllables is then coded for lexical status, either containing an identifiable lexical item or not. Finally, each utterance with a lexical item is coded for multi-word status, whether or not it contains more than one unique word type.

Environmental speech from everyone else is coded for the addressee of each utterance: speech directed to a child, whether or not it is directed to the target child; adult-directed speech; speech directed to both an adult and a child; speech directed to pets or other animals; speech with an unclear addressee; or speech directed towards a recipient that doesn't fit into another category (e.g. voice control of Siri or Alexa, prayer to a metaphysical entity).

# Results

## Measuring Properties of Language Input

### Language Input Quantity

```{r compare-AWC}
compare_AWC <- wilcox.test(LENA_counts$AWC ~ LENA_counts$group)
AWC_stats <- LENA_counts %>% 
  group_by(group) %>%
  summarise(min = min(AWC),
            max = max(AWC),
            mean = mean(AWC))

compare_MWC <- wilcox.test(manual_word_TTR$tokens ~ manual_word_TTR$group)
MWC_stats <- manual_word_TTR %>% 
  group_by(group) %>%
  summarise(min = min(tokens),
            max = max(tokens),
            mean = mean(tokens))
```

We first compare the quantity of language input to blind and sighted children, using LENA's automated Adult Word Count measure. A wilcoxon rank-sum test shows that despite wide variability in the number of words children hear (Range: `r round((AWC_stats %>% filter(group=="VI"))$min)`--`r round((AWC_stats %>% filter(group=="VI"))$max)` words~blind~, `r round((AWC_stats %>% filter(group=="TD"))$min)`--`r round((AWC_stats %>% filter(group=="TD"))$max)` words~sighted~), blind and sighted children do not differ in language input quantity (W = `r compare_AWC$statistic`, *p* `r printp(compare_AWC$p.value, add_equals=TRUE)`). If we instead measure this using word counts from the transcriptions of the audio recordings, we find parallel results: blind and sighted children do not differ in language input quantity (W = `r compare_MWC$statistic`, *p* `r printp(compare_MWC$p.value, add_equals=TRUE)`); see Figure \@ref(fig:quantity-plots). 

```{r quantity-plots, fig.cap="Comparing LENA-generated adult word counts (left) and transcription-based word counts in the input of blind and sighted children. Each dot represents the estimated number of words in one child's recording."}
AWC_plot <-
  ggplot(data = LENA_counts, aes(
    x = group,
    y = AWC,
    color = group,
    fill = group)) +
  geom_violin(alpha = .5, trim = FALSE) +
  geom_jitter(width = 0.2, height = 0) +
  theme_classic() +
  labs(y = "Adult Word Count", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(
    fun = mean,
    geom = "point",
    color = 'black',
    alpha = 0.8) +
  geom_linerange(aes(
    ymin = (mean(AWC) - std.error(AWC)),
    ymax = (mean(AWC) + std.error(AWC)))) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted")) +
  coord_cartesian(ylim = c(0, (max(LENA_counts$AWC * 1.1)))) +
  theme(text = element_text(size = 10)) 

manual_word_count_plot <-  ggplot(data = manual_word_TTR, aes(
    x = group,
    y = tokens,
    color = group,
    fill = group)) +
  geom_violin(alpha = .5, trim = FALSE) +
  geom_jitter(width = 0.2, height = 0) +
  theme_classic() +
  labs(y = "Manual Word Counts", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(
    fun = mean,
    geom = "point",
    color = 'black',
    alpha = 0.8) +
  geom_linerange(aes(
    ymin = (mean(tokens) - std.error(tokens)),
    ymax = (mean(tokens) + std.error(tokens)))) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted")) +
  coord_cartesian(ylim = c(0, (max(manual_word_TTR$tokens * 1.1)))) +
  theme(text = element_text(size = 10)) 

cowplot::plot_grid(AWC_plot, manual_word_count_plot)
```

### Language Input Quality

#### Interactiveness

We compared the proportions of child-directed speech (CDS) between the blind children and their sighted matches. Each proportion was calculated as the number of utterances produced by someone *other* than the target child (non-CHI utterances) tagged with a child or an adult addressee, respectively, out of the total number of non-CHI utterances for each sensory group. A two-sample test for equality of proportions revealed no significant difference in the overall proportions of CDS to blind children and CDS to sighted children.

```{r compare-interactiveness}
CTC_normality_check <- shapiro.test(LENA_counts$CTC)
compare_CTC <- wilcox.test(LENA_counts$CTC ~ LENA_counts$group)
CTC_stats <- LENA_counts %>% 
  group_by(group) %>%
  summarise(min = min(CTC),
            max = max(CTC),
            mean = mean(CTC))

CDS_normality_check <- shapiro.test(xds_props_wide$prop_CDS)
compare_CDS <- wilcox.test(xds_props_wide$prop_CDS ~ xds_props_wide$group)
CDS_stats <- xds_props_wide %>% 
  group_by(group) %>%
  summarise(min = min(prop_CDS),
            max = max(prop_CDS),
            mean = mean(prop_CDS))
```

We next compare the number of conversational turn counts for blind and sighted children, using LENA's automated Conversational Turn Count measure. A Wilcoxon rank-sum test shows that despite wide variability in the number conversational turns (`r round((CTC_stats %>% filter(group=="VI"))$min)`--`r round((CTC_stats %>% filter(group=="VI"))$max)` words~blind~, `r round((CTC_stats %>% filter(group=="TD"))$min)`--`r round((CTC_stats %>% filter(group=="TD"))$max)` words~sighted~), blind and sighted children do not differ in the number of conversational turns (W = `r compare_CTC$statistic`, *p* `r printp(compare_CTC$p.value, add_equals=TRUE)`).

```{r interactiveness-plots, fig.cap="Comparing LENA-generated conversational turn counts (left) and proportion of utterances in child-directed speech (center). Each dot represents one child's recording. The full breakdown by addressee is shown in the rightmost panel.", fig.width=7, fig.height=3}
CTC_plot <-
  ggplot(data = LENA_counts, aes(
    x = group,
    y = CTC,
    color = group,
    fill = group)) +
  geom_violin(alpha = .5, trim = FALSE) +
  geom_jitter(width = 0.2, height = 0) +
  theme_classic() +
  labs(y = "Conversational Turn Count", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(
    fun = mean,
    geom = "point",
    color = 'black',
    alpha = 0.8) +
  geom_linerange(aes(
    ymin = (mean(CTC) - std.error(CTC)),
    ymax = (mean(CTC) + std.error(CTC)))) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted")) +
  coord_cartesian(ylim = c(0, (max(LENA_counts$CTC * 1.1)))) +
  theme(text=element_text(size=14)) 

CDS_prop_plot <- xds_props %>% 
  filter(addressee == "CDS") %>%
  ggplot(aes(
    x = group,
    y = prop,
    color = group,
    fill = group)) +
  geom_violin(alpha = .5, trim = FALSE) +
  geom_jitter(width = 0.2, height = 0) +
  theme_classic() +
  labs(y = "Proportion of\nChild-Directed Speech", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(
    fun = mean,
    geom = "point",
    color = 'black',
    alpha = 0.8) +
  geom_linerange(aes(
    ymin = (mean(prop) - std.error(prop)),
    ymax = (mean(prop) + std.error(prop)))) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted")) +
  coord_cartesian(ylim = c(0, 1)) +
  theme(text=element_text(size=14)) 


addressee_props_plot <- xds_props %>% 
  group_by(group, addressee) %>%
  summarise(prop = mean(prop,na.rm=TRUE)) %>%
  ggplot(aes(x=group,y=prop,fill=factor(addressee,levels=c("UDS","TDS","PDS","ODS","BDS","ADS","CDS")))) +
  geom_bar(stat="identity") +
  scale_fill_manual(name = "Addressee", breaks = c("CDS","ADS","BDS","ODS","PDS","TDS","UDS"),
                    labels = c("Child","Adult","Child & Adult", "Other", "Pet", "TDS", "unknown"),
                    values = c("#FF70AE","#EF233C","#FFB870","#FFF170", "#BCE784","#ade7f5","#b16fff")
                    ) +
    geom_text(aes(label=case_when(prop>.05~round(prop,2))), 
            position=position_stack(vjust=0.5)) +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted"))+
  theme_classic() +
  theme(text=element_text(size=14),
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("Proportion of Utterances\nBy Addressee")+
  xlab(NULL)
  
cowplot::plot_grid(CTC_plot, CDS_prop_plot, addressee_props_plot, rel_widths = c(1,1.1, 2),nrow=1)
```


#### Linguistic Features

```{r linguistic-plots}
TTR_plot <-  ggplot(data = (manual_word_TTR), aes(
    x = group,
    y = TTR,
    color = group,
    fill = group)) +
  geom_violin(alpha = .5, trim = FALSE) +
  geom_jitter(width = 0.2, height = 0) +
  theme_classic() +
  labs(y = "Type-Token Ratio:\nUnique Words / Total Words", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Typically-\nDeveloping", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(
    fun = mean,
    geom = "point",
    color = 'black',
    alpha = 0.8) +
  geom_linerange(aes(
    ymin = (mean(TTR) - std.error(TTR)),
    ymax = (mean(TTR) + std.error(TTR)))) +
  stat_summary(fun.data = "mean_cl_boot", color = 'black') +
  scale_x_discrete(
    labels = c(
      "VI" = "Blind",
      "TD" = "Sighted")) +
  coord_cartesian(ylim = c(0, (max(manual_word_TTR$TTR * 1.1)))) +
  theme(text = element_text(size = 10)) 

shapiro.test(manual_word_TTR$TTR)
TTR_test <- t.test(manual_word_TTR$TTR~manual_word_TTR$group)
TTR_test

MLU_plot <-
  ggplot(data = (MLUs %>% filter(MLU>0)), aes(
    x = group,
    y = MLU,
    color = group,
    fill = group)) +
  geom_violin(alpha = .5, trim = FALSE) +
  geom_jitter(width = 0.2, height = 0) +
  theme_classic() +
  labs(y = "Mean Length of Utterance\n
  (morphemes)", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none") +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(
    fun = mean,
    geom = "point",
    color = 'black',
    alpha = 0.8) +
  geom_linerange(aes(
    ymin = (mean(MLU) - std.error(MLU)),
    ymax = (mean(MLU) + std.error(MLU))), color="black")+
  scale_x_discrete(
    labels = c(
      "TD" = "Sighted",
      "VI" = "Blind")) +
  coord_cartesian(ylim = c(0, (max(MLUs$MLU * 1.1)))) +
  theme(text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1)) 

MLU_stats <- MLUs %>% 
  group_by(group) %>%
  summarise(mean=mean(MLU),
            SE = std.error(MLU))
shapiro.test(MLUs$MLU)
MLU_compare <- wilcox.test(MLUs$MLU ~ MLUs$group)

cowplot::plot_grid(MLU_plot, TTR_plot)
```

For linguistic features, we first measure proportion of unique words divided by the number of total words in the input, or type-token ratio, from the manual annotations. In order to compare the type-token ratio between blind and sighted children, we performed a Wilcoxon signed-rank test. The results of the Wilcoxon test indicated that there was no significant difference in the type-token ratio between the two groups (W = `r TTR_test$statistic`, p `r papaja::printp(TTR_test$p.value, add_equals=TRUE)`). This suggests that, on average, the type-token ratio is similar for blind (M: `r mean((manual_word_TTR%>%filter(group=="VI"))$TTR)`) and sighted (M: `r mean((manual_word_TTR%>%filter(group=="TD"))$TTR)`) children (see Figure \@ref(fig:TTR)). These results provide evidence that the variety of words in the input is not affected by children's vision.

##### MLU Analysis and results

For our second linguistic feature, we analyzed the mean length of utterance (MLU) in the input speech to each group of children. We computed MLU on a morpheme level as a proximate measure of syntactic complexity. Each utterance by a non-CHI speaker was tokenized into morphemes using the 'morphemepiece' R package [CITE]. We then calculated the mean MLU per speaker from each audio recording, and then compared the mean MLU of environmental speech to blind children (M(SD) = XXX (XXX)) to that of sighted children (M(SD) = XXX (XXX)). A Wilcoxon rank-sum test revealed that the MLU was slightly but significantly higher in speech to blind children than to their sighted peers (W = `r MLU_compare$w.value`, *p* = `r printp(MLU_compare$p.value)`).


#### Conceptual Features

topic diversity - <https://github.com/mind-Lab/octis> 
sensory modality 
temporality

```{r sensory-modality}

visual_plot <- ggplot(data = sensory_props_wide, aes(
    x = group,
    y = prop_Visual,
    color = group,
    fill = group)) +
    geom_jitter(width = 0.2, height = 0) +
  geom_violin(alpha = .5, trim = FALSE) +
  theme_classic() +
  labs(y = "Proportion of\nHighly Visual Words", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none") +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(
    fun =  mean,
    geom = "point",
    color = 'black',
    alpha = 0.8) +
  geom_linerange(aes(
    ymin = (mean(prop_Visual) - std.error(prop_Visual)),
    ymax = (mean(prop_Visual) + std.error(prop_Visual))), color="black")+
  scale_x_discrete(
    labels = c(
      "TD" = "Sighted",
      "VI" = "Blind")) +
  coord_cartesian(ylim = c(0, .5)) +
  theme(text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1)) 

sensory_props_plot <- sensory_props %>% 
  group_by(group,Modality) %>%
  summarise(prop = mean(prop,na.rm=TRUE)) %>%
  ggplot(aes(fill = factor(Modality,levels=c("Amodal","Olfactory","Gustatory","Haptic","Interoceptive","Auditory","Visual","Multimodal")), 
             y= prop, 
             x = group)) +
  geom_bar(position = "stack", stat = "identity")  +
  geom_text(aes(label=case_when(prop>.05~round(prop,2))), 
            position=position_stack(vjust=0.5)) +
  xlab("Group") + 
  ylab ("Proportion of words\nby sensory modality") +
  theme_classic()+
  theme(text=element_text(size=10),
    axis.text.x = element_text(angle = 45, hjust = 1)
        ) + 
  scale_fill_manual(name="Modality",
                    breaks=c("Multimodal","Visual","Auditory","Interoceptive","Haptic","Gustatory","Olfactory", "Amodal"),
                    values=c("#FF70AE","#EF233C","#FFB870","#FFF170", "#BCE784","#ade7f5","#b16fff","#A5ACA5"))+
  scale_x_discrete(labels=c("VI" = "Blind", "TD" = "Sighted")) +
  xlab(NULL) 
# then do this same graph, but by target child only   

```

```{r verb-tense}
n_uncat<-sum(verbs_only$temporality == "uncategorized")

hereandnow_plot <- ggplot(data = temporality_props_wide, aes(x = group,
    y = prop_present,
    color = group,
    fill = group)) +
    geom_jitter(width = 0.2, height = 0) +
  geom_violin(alpha = .5, trim = FALSE) +
  theme_classic() +
  labs(y = "Proportion of\nPresent Verbs", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none") +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(
    fun = mean,
    geom = "point",
    color = 'black',
    alpha = 0.8) +
  geom_linerange(aes(
    ymin = (mean(prop_present) - std.error(prop_present)),
    ymax = (mean(prop_present) + std.error(prop_present))), color="black")+
  scale_x_discrete(
    labels = c(
      "TD" = "Sighted",
      "VI" = "Blind")) +
  coord_cartesian(ylim = c(0, 1)) +
  theme(text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1))


temporality_props_plot <- temporality_props %>%
  mutate(group=as.factor(str_sub(VIHI_ID,1,2))) %>% 
  group_by(group, verb_temporality) %>%
  summarise(prop = mean(prop, na.rm = TRUE)) %>%
  ggplot(aes(fill = factor(verb_temporality, levels=c("uncategorized","displaced","present")),
                                 y = prop,
                                 x = group)) +
  geom_bar(position = "stack", stat = "identity")  +
  ylab ("Proportion of utterances\nby verb temporality") +
   geom_text(aes(label=round(prop,2)), 
            position=position_stack(vjust=0.5)) +
  theme_classic() +
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(name="Verb Tense",
                    labels=c("uncategorized", "displaced", "present"),
                    values=c("#A5ACA5","#FFF170","#BCE784"))+
  scale_x_discrete(labels =                                                   c("VI" = "Blind", "TD" = "Sighted")) +
  xlab(NULL) 
```
```{r temporality-stats}
temporality_compare <- mutate(verbs_only, group=as.factor(str_sub(doc_id,1,2)))
prop.test(x=sum(temporality_compare, temporality="present", group_by(temporality_compare, group)), n=sum(temporality_compare, group_by(verbs_only, group)))

```
The last conceptual feature we examined is the displacement of events discussed in children's linguistic environment. That is, we tried to ascertain whether blind and sighted children receive a different amount of "here-and-now" commentary related to the events happening around them. Prior work has quantified such "here-and-nowness" by counting object presence co-occurring with a related noun label [CITE]. The audio format of our data and the coding scheme we use make it difficult to ascertain object presence, so instead of assessing objects' spacial displacement, in this analysis we will focus on the temporal displacement. Notably, we are attempting to highlight semantic features of the language environment; however, given the constraints of large-scale textual analysis, we are categorizing utterances based on a combination of closely related syntactic and morphological features of verbs, since these express time-relevant information. We recognize that these linguistic features do not perfectly align with the temporal structure of the world.

We assigned each input utterance a **temporality** value: utterances tagged *displaced* describe events that take place in the past, future, or irrealis space, while utterances tagged *present* describe current, ongoing events. A small amount of utterances (n = `r n_uncat`) were left *uncategorized* because they were fragments or because the automated parser failed to tag any of the relevant features. To do this, we used the udpipe R package [CITE] to tag the transcriptions with parts of speech and other lexical features, such as tense, number agreement, or case inflection.  To be marked as present, an utterance either had to be marked with both present tense and indicative mood, or appear in the gerund form with no marked tense (e.g. *you talking to Papa?*). Features that could mark an utterance as displaced included past tense, imperative mood, presence of a modal, presence of *if*, or presence of *gonna*/*going to*, *have to*, *wanna*/*want to*, or *gotta*/*got to*, since these typically indicate belief states and desires, rather than real-time events. In the case of utterances with multiple verbs, we selected the features from the first verb or auxiliary, as a proxy for hierarchical dominance, since these generally carry tense features.

The proportion of present versus displaced temporality did not differ between groups ().

```{r cboi}
CBOI_plot <-
  ggplot(data = subj_CBOI_means, aes(
    x = group,
    y = CBOI_Mean,
    color = group,
    fill = group)) +
    geom_jitter(width = 0.2, height = 0) +
  geom_violin(alpha = .5, trim = FALSE) +
  theme_classic() +
  labs(y = "Mean Child-Body-Object\nInteraction Rating (1-7)", x = NULL, element_text(size = 10)) +
  theme(
    legend.title = element_blank(),
    legend.position = "none") +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  stat_summary(
    fun = mean,
    geom = "point",
    color = 'black',
    alpha = 0.8) +
  geom_linerange(aes(
    ymin = (mean(CBOI_Mean) - std.error(CBOI_Mean)),
    ymax = (mean(CBOI_Mean) + std.error(CBOI_Mean))), color="black")+
  scale_x_discrete(
    labels = c(
      "TD" = "Sighted",
      "VI" = "Blind")) +
  coord_cartesian(ylim = c(1, 7)) +
  theme(text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1)) 

CBOI_distribution_plot <- content_words_only %>%
  ggplot(aes(x=CBOI_Mean, color=group, fill=group)) +
  geom_density(alpha=.5) +
  theme_classic() +
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  xlab("Child-Body-Object Interaction Rating")+
  theme(text = element_text(size = 10)) + 
  annotate(geom="text", x=2, y=.5, label="hire,\nkill,\nfamous") +
  annotate(geom="text", x=4.5, y=.5, label="yawn,\nstop,\nrecorder") +
  annotate(geom="text", x=6.5, y=.5, label="dog,\ncar,\nspaghetti") +
  geom_segment(aes(x = 2, xend=2,y = .4, yend = .2),arrow = arrow(), color="black") +
  geom_segment(aes(x = 4.5, xend=4.5,y = .4, yend = .2),arrow = arrow(), color="black") +
  geom_segment(aes(x = 6.5, xend=6.5,y = .4, yend = .2),arrow = arrow(), color="black") +
  ylab("Content Word Density")
```

```{r conceptual-plots, fig.height=8, fig.width = 7}
cowplot::plot_grid(visual_plot, sensory_props_plot,
                   hereandnow_plot, temporality_props_plot,
                   CBOI_plot, CBOI_distribution_plot,
                   nrow=3, rel_widths = c(1,1.5))
```

## Linking Language Input to Language Outcomes

Predict: CDI percentile & CVC percentile

```{r predicting-plots, fig.height=6,fig.width=6,fig.cap="Plots of the relationship between the input variables and children's vocabulary outcomes (relative to the Wordbank 50th percentile."}
VIHI_CDI <- read.csv("/Users/eec35/Desktop/Git/LENA/input_quality/data/CDI/Wordbank/VIHI_CDI.csv") %>% 
dplyr::rename(CDI_age_in_days=age) 

lotta_data <- LENA_counts %>%
  left_join(MLUs, by="VIHI_ID")  %>%
  left_join(manual_word_TTR, by="VIHI_ID") %>%
  left_join(xds_props_wide %>% select(-total), by=c("VIHI_ID", "group")) %>%
  left_join(sensory_props_wide %>% select(-total), by=c("VIHI_ID", "group"))%>% 
  left_join(subj_CBOI_means, by=c("VIHI_ID", "group")) %>%
  left_join(temporality_props_wide, by=c("VIHI_ID", "group")) %>%
  mutate(ParticipantNumber = as.factor(str_sub(VIHI_ID, 1, 6))) %>%
  mutate(LENA_age_in_days = as.numeric(str_sub(VIHI_ID, 8, length(VIHI_ID))))%>%
  left_join(VIHI_CDI, by=c("group", "ParticipantNumber")) %>%
  mutate(CDI_LENA_days_apart = abs(CDI_age_in_days - LENA_age_in_days),
         group = as.factor(group)) %>%
  arrange(CDI_LENA_days_apart) %>%
  distinct(ParticipantNumber, .keep_all=TRUE) 

predict_CDI <- lm(diff_age_from_expected ~ (CDI_age_in_days*group + tokens*group + TTR*group + CTC*group + prop_CDS*group + MLU*group + prop_present*group + prop_Visual*group + CBOI_Mean*group), data = lotta_data)

# summary(predict_CDI)
# MASS::stepAIC(predict_CDI)
AIC_selected <- lm(formula = diff_age_from_expected ~ CDI_age_in_days + MLU + 
    prop_present, data = lotta_data)
summary(AIC_selected)

predict_CDI_TD <- lm(diff_age_from_expected ~  CDI_age_in_days + AWC + tokens + TTR + CTC + prop_CDS + MLU + prop_present + prop_Visual + CBOI_Mean, data = (lotta_data %>% filter(group=="TD")))
# car::vif(predict_CDI_TD)
# MASS::stepAIC(predict_CDI_TD)
AIC_selected_TD <- lm(formula = diff_age_from_expected ~ tokens + CTC + prop_CDS + 
    MLU, data = (lotta_data %>% filter(group == "TD")))
# summary(AIC_selected_TD)
predict_CDI_VI <- lm(diff_age_from_expected ~ CDI_age_in_days + AWC + tokens + TTR + CTC + prop_CDS + MLU + prop_present + prop_Visual + CBOI_Mean, data = (lotta_data %>% filter(group=="VI")))
# MASS::stepAIC(predict_CDI_VI)
AIC_selected_VI <- lm(formula = diff_age_from_expected ~ AWC + CTC + prop_CDS + 
    MLU, data = (lotta_data %>% filter(group == "VI")))
# summary(AIC_selected_VI)



AWC_predict_vocab <- ggplot((lotta_data %>% filter(!is.na(group))), aes(x=AWC, y=diff_age_from_expected, color=group, fill=group)) +
  geom_point()+
  geom_smooth(method="lm")+
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5"))+
  theme_classic()+
  theme(legend.position = "none")+
  ylab(NULL)
TTR_predict_vocab <- ggplot((lotta_data %>% filter(!is.na(group))), aes(x=TTR, y=diff_age_from_expected, color=group, fill=group)) +
  geom_point()+
  geom_smooth(method="lm")+
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5"))+
  theme_classic()+
  theme(legend.position = "none")+
  ylab(NULL)
CTC_predict_vocab <- ggplot((lotta_data %>% filter(!is.na(group))), aes(x=CTC, y=diff_age_from_expected, color=group, fill=group)) +
  geom_point()+
  geom_smooth(method="lm")+
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5"))+
  theme_classic()+
  theme(legend.position = "none")+
  ylab(NULL)
CDS_predict_vocab <- ggplot((lotta_data %>% filter(!is.na(group))), aes(x=prop_CDS, y=diff_age_from_expected, color=group, fill=group)) +
  geom_point()+
  geom_smooth(method="lm")+
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5"))+
  theme_classic()+
  theme(legend.position = "none")+
  ylab(NULL)
visual_predict_vocab <- ggplot((lotta_data %>% filter(!is.na(group))), aes(x=prop_Visual, y=diff_age_from_expected, color=group, fill=group)) +
  geom_point()+
  geom_smooth(method="lm")+
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5"))+
  theme_classic()+
  theme(legend.position = "none")+
  ylab(NULL)
CBOI_predict_vocab <- ggplot((lotta_data %>% filter(!is.na(group))), aes(x=CBOI_Mean, y=diff_age_from_expected, color=group, fill=group)) +
  geom_point()+
  geom_smooth(method="lm")+
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5"))+
  theme_classic()+
  theme(legend.position = "none")+
  ylab(NULL)
  
MLU_predict_vocab <- ggplot((lotta_data %>% filter(!is.na(group))), aes(x=MLU, y=diff_age_from_expected, color=group, fill=group)) +
  geom_point()+
  geom_smooth(method="lm")+
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5"))+
  theme_classic()+
  ylab("Vocabulary Delay (months)")
temporality_predict_vocab <- ggplot((lotta_data %>% filter(!is.na(group))), aes(x=prop_present, y=diff_age_from_expected, color=group, fill=group)) +
  geom_point()+
  geom_smooth(method="lm")+
  scale_color_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5")) +
  scale_fill_manual(
    breaks = c("TD", "VI"),
    labels = c("Sighted", "Blind"),
    values = c("#b16fff", "#ade7f5"))+
  theme_classic()+
  theme(legend.position = "none")+
  ylab(NULL)

dont_predict_plots <- cowplot::plot_grid(AWC_predict_vocab, TTR_predict_vocab, CTC_predict_vocab, CDS_predict_vocab, ncol=1)
cowplot::plot_grid(dont_predict_plots, MLU_predict_vocab, nrow=1, rel_widths = c(1,3))


```

```{r variable-relationships}
VI_cors <- cor(lotta_data %>% filter(group=="VI") %>% select(MLU,AWC,tokens,CTC,prop_CDS,TTR,CBOI_Mean, prop_present, prop_Visual))
TD_cors <- cor(lotta_data %>% filter(group=="TD") %>% select(MLU,AWC,tokens,CTC,prop_CDS,TTR,CBOI_Mean, prop_present, prop_Visual))
corrplot(VI_cors, type = "upper",method="color")
corrplot(TD_cors, type = "lower",method="color")
```

# Discussion

Sighted parents may be unfamiliar with blind children's signals of interest and engagement [@perez-pereira1999], and as a result, may respond less often to infants' vocalizations and bids for communication [@rowland1984]. Might be hard to provide useful input due to differences in nonverbal communication between blind infants and their sighted caregivers. Young children born with visual impairment may differ in their nonverbal communication cues. For example, [@preisler1995] found that 6--9-month-old blind infants communicated using leaning, eyebrow raising, and lip movements. Caregivers who responded to these nonverbal cues as conversational turns had higher rates of interaction with the child, higher rates of appropriate response, and increased positive affect. By contrast, caregivers who did not recognize these signals as communicative had lower rates of response and increased negative affect.

\pagebreak

# References
